{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_binary.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOm80CPMPgvXjbg9kb7DfZA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daldric/MNIST-Machine-Learning/blob/main/MNIST_binary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYPgd4YP4PqX"
      },
      "source": [
        "Data Ingestion -> Model -> Output -> update model -> ?\n",
        "\n",
        "Step 1 - import the needed libraries\n",
        "Step 2 - load the data\n",
        "Step 3 - visualize the data\n",
        "Step 4 - data preprocessing and data augmentation\n",
        "Step 5 - define the model\n",
        "Step 6 - evaluating the result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg3X7kPahhF0"
      },
      "source": [
        "# Step 1 - import the needed libraries\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import statistics\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "from utils import read_data\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "463z43X1LRa-"
      },
      "source": [
        "# importing my images, adding them to a 2d list with their labels\n",
        "uploaded = files.upload()\n",
        "myImages = []\n",
        "img = cv2.imread('three.png', cv2.IMREAD_GRAYSCALE)\n",
        "img2 = cv2.imread('seven.png', cv2.IMREAD_GRAYSCALE)\n",
        "img = cv2.bitwise_not(img)\n",
        "img = img.astype(float)\n",
        "img2 = cv2.bitwise_not(img2)\n",
        "img2 = img2.astype(float)\n",
        "myImages.append([img, 0])\n",
        "myImages.append([img2, 1])\n",
        "plt.clf()\n",
        "fig=plt.figure()\n",
        "fig.add_subplot(2, 2, 1)\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "fig.add_subplot(2, 2, 2)\n",
        "plt.imshow(img2)\n",
        "plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjHMcHQLUBtf"
      },
      "source": [
        "print(img.shape)\n",
        "myImages = [[img, 0], [img2, 1]]\n",
        "print(len(myImages[0][0]))\n",
        "for x in range(len(myImages)):\n",
        "  for row in range(len(myImages[x][0])):\n",
        "    for col in range(len(myImages[x][0][row])):\n",
        "      if myImages[x][0][row][col] == 255:\n",
        "        myImages[x][0][row][col] = 1\n",
        "      # print(myImages[x][0][row][col].item())\n",
        "  myImages[x][0] = torch.tensor(myImages[x][0])\n",
        "  myImages[x][0] = myImages[x][0].squeeze(0)\n",
        "  myImages[x][0] = myImages[x][0].unsqueeze(0)\n",
        "  \n",
        "      \n",
        "\n",
        "print(type(myImages[0][0][0][0][0].item())) # needs to be type float, not byte (or int, whatever)\n",
        "print(myImages[0][0].shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fsdb3cqYniJW"
      },
      "source": [
        "# custom datasets so that the indices can be reassigned to make the loader binary\n",
        "class Training(Dataset):\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.data=[]\n",
        "    training = torchvision.datasets.MNIST(root=\"./\", train=True, download=True, transform=ToTensor())\n",
        "    test = False\n",
        "    for x in range(len(training)):\n",
        "      if training[x][1]==3:\n",
        "        l=list(training[x])\n",
        "        l[1]=0\n",
        "        l=tuple(l)\n",
        "        self.data.append(l)\n",
        "        if test:\n",
        "          print(l)\n",
        "          test=False\n",
        "      elif training[x][1]==7:\n",
        "        l=list(training[x])\n",
        "        l[1]=1\n",
        "        l=tuple(l)\n",
        "        self.data.append(l)\n",
        "      '''\n",
        "      else:\n",
        "        l=list(training[x])\n",
        "        l[1]=2\n",
        "        l=tuple(l)\n",
        "        self.data.append(l)\n",
        "      '''\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx]\n",
        "  \n",
        "\n",
        "class Testing(Dataset):\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.data=[]\n",
        "    testing = torchvision.datasets.MNIST(root=\"./\", train=False, download=True, transform=ToTensor())\n",
        "    test = False\n",
        "    for x in range(len(testing)):\n",
        "      if testing[x][1]==3:\n",
        "        l=list(testing[x])\n",
        "        l[1]=0\n",
        "        l=tuple(l)\n",
        "        self.data.append(l)\n",
        "        if test:\n",
        "          print(l)\n",
        "          test=False\n",
        "      elif testing[x][1]==7:\n",
        "        l=list(testing[x])\n",
        "        l[1]=1\n",
        "        l=tuple(l)\n",
        "        self.data.append(l)\n",
        "      '''\n",
        "      else:\n",
        "        l=list(testing[x])\n",
        "        l[1]=2\n",
        "        l=tuple(l)\n",
        "        self.data.append(l)\n",
        "      '''\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx]\n",
        "\n",
        "class Custom(Dataset):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.data=[]\n",
        "    for i in range(len(myImages)):\n",
        "      l=myImages[i]\n",
        "      l=tuple(l)\n",
        "      self.data.append(l)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCnDBd2qqMaO"
      },
      "source": [
        "training_data = Training()\n",
        "\n",
        "testing_data = Testing()\n",
        "\n",
        "custom_data = Custom()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Eki9h1BsjHF"
      },
      "source": [
        "# not needed to update all the information\n",
        "print(len(training_data))\n",
        "print(training_data[0][1])\n",
        "image = training_data[0][0].numpy()\n",
        "image = image.squeeze()\n",
        "imgplot = plt.imshow(image)\n",
        "\n",
        "print(len(testing_data))\n",
        "print(testing_data[0][1])\n",
        "print(testing_data[0][0].shape)\n",
        "print(testing_data[0][0])\n",
        "print(type(testing_data[0][0][0][0][0].item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwkTjqcmUCaJ"
      },
      "source": [
        "# not needed to update the information\n",
        "# normalize the data with the mean and std\n",
        "std = 0\n",
        "sum = 0\n",
        "divisor = 0\n",
        "mean = 0\n",
        "for x in range(len(training_data)):\n",
        "  mean += training_data[x][0].mean().item()\n",
        "mean = mean / len(training_data)\n",
        "\n",
        "for x in range(len(training_data)):\n",
        "  a = training_data[x][0].numpy()\n",
        "  for y in range(28):\n",
        "    for z in range(28):\n",
        "      sum += math.pow(a[0][y][z] - mean, 2)\n",
        "      divisor+=1\n",
        "\n",
        "std = math.sqrt(sum / divisor)\n",
        "\n",
        "\n",
        "print(mean) # mean is 0.1279 when binary, 0.1307 when using the whole dataset\n",
        "print(std) # std is 0.3052 when binary, 0.3081 when using the whole dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyY2Ml1jONNS"
      },
      "source": [
        "# not needed to update the information\n",
        "# the mean and std of the custom dataset\n",
        "std = 0\n",
        "sum = 0\n",
        "divisor = 0\n",
        "mean = 0\n",
        "for x in range(len(custom_data)):\n",
        "  mean += custom_data[x][0].mean().item()\n",
        "mean = mean / len(custom_data)\n",
        "\n",
        "for x in range(len(custom_data)):\n",
        "  a = custom_data[x][0].numpy()\n",
        "  for y in range(28):\n",
        "    for z in range(28):\n",
        "      sum += math.pow(a[0][y][z] - mean, 2)\n",
        "      divisor+=1\n",
        "\n",
        "std = math.sqrt(sum / divisor)\n",
        "\n",
        "\n",
        "print(mean) # mean is 0.1441\n",
        "print(std) # std is 0.3512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lHA5pw2tymk"
      },
      "source": [
        "data_transforms = transforms.Normalize(0.1279, 0.3052)\n",
        "custom_transforms = transforms.Normalize(0.1441, 0.3512)\n",
        "training_data.transform = data_transforms\n",
        "testing_data.transform = data_transforms\n",
        "custom_data.transform = custom_transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw3Ouho46HQQ"
      },
      "source": [
        "# make the data loaders\n",
        "training_loader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "testing_loader = DataLoader(testing_data, batch_size=64, shuffle=True)\n",
        "custom_loader = DataLoader(custom_data, batch_size=len(custom_data), shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlurZnBcGFrj"
      },
      "source": [
        "# create a method to find the max indices of the tensor outputs\n",
        "def batch_correct_num(data, values):\n",
        "  num_correct = 0\n",
        "  for x in range(len(values)):\n",
        "    if torch.argmax(data, dim=1)[x]==values[x].item():\n",
        "      num_correct+=1\n",
        "  return num_correct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OslPbbiMG8sT"
      },
      "source": [
        "# Now make the model as a class\n",
        "class Network(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Linear(784, 128), # input is the dimension of the image\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(128, 64),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 2) # will eventually be three outputs, 0 is 3, 1 is 7, 2 is neither\n",
        "    )\n",
        "    \n",
        "  def forward(self, data):\n",
        "    x = data.view(data.size(0), -1) # 64 x 784\n",
        "    output = self.layers(x)\n",
        "    return output\n",
        "\n",
        "n = Network()\n",
        "print(n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9buQinHAr65"
      },
      "source": [
        "# Pass training data into the model\n",
        "optimizer = optim.Adam(n.parameters(), lr=1e-5) # learning rate is hyperparameter that should be played with\n",
        "EPOCHS = 10 # each epoch is a pass through all the data\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "loss_graphing = []\n",
        "accuracy_graphing = []\n",
        "test = False\n",
        "epochs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "for epoch in range(EPOCHS):\n",
        "  epoch_loss = 0\n",
        "  num_correct = 0\n",
        "  for data_X, data_y in training_loader:\n",
        "    optimizer.zero_grad()\n",
        "    output = n(data_X)\n",
        "    loss = loss_func(output, data_y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(loss)\n",
        "    epoch_loss += loss.item()\n",
        "    num_correct += batch_correct_num(output, data_y)\n",
        "    if test:\n",
        "      print(output.shape)\n",
        "      print(data_y)\n",
        "      print(data_y.shape)\n",
        "      print(output)\n",
        "      print(len(output))\n",
        "      print(predictions)\n",
        "      print(len(predictions))\n",
        "      test = False\n",
        "    \n",
        "  print(\"Epoch {}: Training Loss: {:.5f} Accuracy: {}/{}\".format(epoch+1, epoch_loss, num_correct, len(training_data)))\n",
        "  loss_graphing.append(epoch_loss)\n",
        "  accuracy_graphing.append(num_correct)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHsg93C5RIhk"
      },
      "source": [
        "# try the model on the cutom data\n",
        "fig=plt.figure()\n",
        "num_correct=0\n",
        "for data_X, data_y in custom_loader:\n",
        "  output = n(data_X.float())\n",
        "  num_correct += batch_correct_num(output, data_y)\n",
        "  for x in range(len(output)):\n",
        "    print(output[x])\n",
        "    fig.add_subplot(2, 2, x+1)\n",
        "    plt.imshow(myImages[x][0][0])\n",
        "    plt.axis('off')\n",
        "print('Accuracy: {}/{}'.format(num_correct, len(custom_data)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xB_PE_fuxmPZ"
      },
      "source": [
        "# try the model on the testing data\n",
        "num_correct=0\n",
        "for data_X, data_y in testing_loader:\n",
        "  output = n(data_X.float())\n",
        "  num_correct += batch_correct_num(output, data_y)\n",
        "  print(output)\n",
        "print('Testing accuracy: {}/{}'.format(num_correct, len(testing_data)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHzQDnLOjorv"
      },
      "source": [
        "# not needed to update the data\n",
        "# visualize the results\n",
        "fig=plt.figure()\n",
        "ax = fig.add_axes([0, 0, 1, 1])\n",
        "ax.bar(epochs, height=accuracy_graphing)\n",
        "ax.set_ylabel('Number Correct')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_title('Binary accuracy by epoch with training')\n",
        "ax.set_yticks(np.arange(0, 13001, 1000))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}