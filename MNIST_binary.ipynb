{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_binary.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOm80CPMPgvXjbg9kb7DfZA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daldric/MNIST-Machine-Learning/blob/main/MNIST_binary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYPgd4YP4PqX"
      },
      "source": [
        "Data Ingestion -> Model -> Output -> update model -> ?\n",
        "\n",
        "Step 1 - import the needed libraries\n",
        "Step 2 - load the data\n",
        "Step 3 - visualize the data\n",
        "Step 4 - data preprocessing and data augmentation\n",
        "Step 5 - define the model\n",
        "Step 6 - evaluating the result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg3X7kPahhF0"
      },
      "source": [
        "# Step 1 - import the needed libraries\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import statistics\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "from utils import read_data\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "463z43X1LRa-",
        "outputId": "059494d2-1eff-475a-90d4-72d808daaddc"
      },
      "source": [
        "# importing my images, adding them to a 2d list with their labels\n",
        "uploaded = files.upload()\n",
        "myImages = []\n",
        "img = cv2.imread('three.png', cv2.IMREAD_GRAYSCALE)\n",
        "img2 = cv2.imread('seven.png', cv2.IMREAD_GRAYSCALE)\n",
        "img = cv2.bitwise_not(img)\n",
        "img = img.astype(float)\n",
        "img2 = cv2.bitwise_not(img2)\n",
        "img2 = img2.astype(float)\n",
        "myImages.append([img, 0])\n",
        "myImages.append([img2, 1])\n",
        "plt.clf()\n",
        "fig=plt.figure()\n",
        "fig.add_subplot(2, 2, 1)\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "fig.add_subplot(2, 2, 2)\n",
        "plt.imshow(img2)\n",
        "plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6129990c-73c3-43fe-a497-a78420f9dad6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6129990c-73c3-43fe-a497-a78420f9dad6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving seven.png to seven.png\n",
            "Saving three.png to three.png\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.5, 27.5, 27.5, -0.5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAScAAABxCAYAAACX+mUfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAACzElEQVR4nO3dwY3TUBSF4WREFVRBE4gKqJIKEE1QBWXgWbCxAngSiPP+53zfNqPIi5eje24cz3lZlhNAzcvoCwD4E+EEJAknIEk4AUnCCUgSTkDSu60XP758dp9BxLefX86jr+FInO2Ov51tkxOQJJyAJOEEJAknIEk4AUnCCUgSTkCScAKShBOQJJyApM2frxzB1x/fr/7bT+8/7HglwC1MTkCScAKSpq11t9S1e7ynysdb9jiTNY/8HJicgCThBCQJJyApvXMqdfjLa7GDgn2ZnIAk4QQkDa91e1e3W+pXqUYyn1mrfvXcm5yAJOEEJAknIOnhO6d79ds9+v3le25d6/q1WXcNPK9rP4cjz7bJCUgSTkDSw2vdVnVSj2AfMz500eQEJAknIEk4AUnDf75S6bdwNDPcLrDF5AQkCScgaXitm1V1FIZrzHB+TU5AknACkoQTkGTntFJ9IiBc42jn1+QEJAknIOnpa93sd9HyvGZ80sAtTE5AknACkoQTkDTtzuloX5vCNY6+Z1ozOQFJwglISte6UnW7vJbZR2bm8Ew17pLJCUgSTkCScAKS0jundYcu7Z9OJ/8MlP34SdUvJicgSTgBScIJSErvnNZG9OvanotjeuZ7mbaYnIAk4QQkTVPrRijfysDc3C7wNpMTkCScgCThBCTZOcED2FnezuQEJAknIEmtg538a5V75tsH1kxOQJJwApKEE5A0fOe01ctHd29f//IIo895lckJSBJOQNLwWrelXKuM4lzy0Lj7MjkBScIJSBJOQNLwndMsT5u0I+B/OD+3MzkBScIJSBpe69buNfr6NTgjOD/3ZXICkoQTkCScgKTUzuledH+Yn8kJSBJOQJJwApKEE5AknIAk4QQkCScgSTgBScIJSBJOQJJwApKEE5AknICk87Iso68B4DcmJyBJOAFJwglIEk5AknACkoQTkPQK5ZeHmsDzc/gAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjHMcHQLUBtf",
        "outputId": "213620d9-0da9-47ff-9e2a-16b5741375ce"
      },
      "source": [
        "print(img.shape)\n",
        "myImages = [[img, 0], [img2, 1]]\n",
        "print(len(myImages[0][0]))\n",
        "for x in range(len(myImages)):\n",
        "  for row in range(len(myImages[x][0])):\n",
        "    for col in range(len(myImages[x][0][row])):\n",
        "      if myImages[x][0][row][col] == 255:\n",
        "        myImages[x][0][row][col] = 1\n",
        "      # print(myImages[x][0][row][col].item())\n",
        "  myImages[x][0] = torch.tensor(myImages[x][0])\n",
        "  myImages[x][0] = myImages[x][0].squeeze(0)\n",
        "  myImages[x][0] = myImages[x][0].unsqueeze(0)\n",
        "  \n",
        "      \n",
        "\n",
        "print(type(myImages[0][0][0][0][0].item())) # needs to be type float, not byte (or int, whatever)\n",
        "print(myImages[0][0].shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28, 28)\n",
            "28\n",
            "<class 'float'>\n",
            "torch.Size([1, 28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fsdb3cqYniJW"
      },
      "source": [
        "# custom datasets so that the indices can be reassigned to make the loader binary\n",
        "class Training(Dataset):\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.data=[]\n",
        "    training = torchvision.datasets.MNIST(root=\"./\", train=True, download=True, transform=ToTensor())\n",
        "    test = False\n",
        "    for x in range(len(training)):\n",
        "      if training[x][1]==3:\n",
        "        l=list(training[x])\n",
        "        l[1]=0\n",
        "        l=tuple(l)\n",
        "        self.data.append(l)\n",
        "        if test:\n",
        "          print(l)\n",
        "          test=False\n",
        "      elif training[x][1]==7:\n",
        "        l=list(training[x])\n",
        "        l[1]=1\n",
        "        l=tuple(l)\n",
        "        self.data.append(l)\n",
        "      '''\n",
        "      else:\n",
        "        l=list(training[x])\n",
        "        l[1]=2\n",
        "        l=tuple(l)\n",
        "        self.data.append(l)\n",
        "      '''\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx]\n",
        "  \n",
        "\n",
        "class Testing(Dataset):\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.data=[]\n",
        "    testing = torchvision.datasets.MNIST(root=\"./\", train=False, download=True, transform=ToTensor())\n",
        "    test = False\n",
        "    for x in range(len(testing)):\n",
        "      if testing[x][1]==3:\n",
        "        l=list(testing[x])\n",
        "        l[1]=0\n",
        "        l=tuple(l)\n",
        "        self.data.append(l)\n",
        "        if test:\n",
        "          print(l)\n",
        "          test=False\n",
        "      elif testing[x][1]==7:\n",
        "        l=list(testing[x])\n",
        "        l[1]=1\n",
        "        l=tuple(l)\n",
        "        self.data.append(l)\n",
        "      '''\n",
        "      else:\n",
        "        l=list(testing[x])\n",
        "        l[1]=2\n",
        "        l=tuple(l)\n",
        "        self.data.append(l)\n",
        "      '''\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx]\n",
        "\n",
        "class Custom(Dataset):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.data=[]\n",
        "    for i in range(len(myImages)):\n",
        "      l=myImages[i]\n",
        "      l=tuple(l)\n",
        "      self.data.append(l)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCnDBd2qqMaO"
      },
      "source": [
        "training_data = Training()\n",
        "\n",
        "testing_data = Testing()\n",
        "\n",
        "custom_data = Custom()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Eki9h1BsjHF"
      },
      "source": [
        "# not needed to update all the information\n",
        "print(len(training_data))\n",
        "print(training_data[0][1])\n",
        "image = training_data[0][0].numpy()\n",
        "image = image.squeeze()\n",
        "imgplot = plt.imshow(image)\n",
        "\n",
        "print(len(testing_data))\n",
        "print(testing_data[0][1])\n",
        "print(testing_data[0][0].shape)\n",
        "print(testing_data[0][0])\n",
        "print(type(testing_data[0][0][0][0][0].item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwkTjqcmUCaJ"
      },
      "source": [
        "# not needed to update the information\n",
        "# normalize the data with the mean and std\n",
        "std = 0\n",
        "sum = 0\n",
        "divisor = 0\n",
        "mean = 0\n",
        "for x in range(len(training_data)):\n",
        "  mean += training_data[x][0].mean().item()\n",
        "mean = mean / len(training_data)\n",
        "\n",
        "for x in range(len(training_data)):\n",
        "  a = training_data[x][0].numpy()\n",
        "  for y in range(28):\n",
        "    for z in range(28):\n",
        "      sum += math.pow(a[0][y][z] - mean, 2)\n",
        "      divisor+=1\n",
        "\n",
        "std = math.sqrt(sum / divisor)\n",
        "\n",
        "\n",
        "print(mean) # mean is 0.1279 when binary, 0.1307 when using the whole dataset\n",
        "print(std) # std is 0.3052 when binary, 0.3081 when using the whole dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyY2Ml1jONNS",
        "outputId": "9b6e48fe-fe8d-4115-c874-ed5643773fe4"
      },
      "source": [
        "# not needed to update the information\n",
        "# the mean and std of the custom dataset\n",
        "std = 0\n",
        "sum = 0\n",
        "divisor = 0\n",
        "mean = 0\n",
        "for x in range(len(custom_data)):\n",
        "  mean += custom_data[x][0].mean().item()\n",
        "mean = mean / len(custom_data)\n",
        "\n",
        "for x in range(len(custom_data)):\n",
        "  a = custom_data[x][0].numpy()\n",
        "  for y in range(28):\n",
        "    for z in range(28):\n",
        "      sum += math.pow(a[0][y][z] - mean, 2)\n",
        "      divisor+=1\n",
        "\n",
        "std = math.sqrt(sum / divisor)\n",
        "\n",
        "\n",
        "print(mean) # mean is 0.1441\n",
        "print(std) # std is 0.3512"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.14413265306122447\n",
            "0.351224189632144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lHA5pw2tymk"
      },
      "source": [
        "data_transforms = transforms.Normalize(0.1279, 0.3052)\n",
        "custom_transforms = transforms.Normalize(0.1441, 0.3512)\n",
        "training_data.transform = data_transforms\n",
        "testing_data.transform = data_transforms\n",
        "custom_data.transform = custom_transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw3Ouho46HQQ"
      },
      "source": [
        "# make the data loaders\n",
        "training_loader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "testing_loader = DataLoader(testing_data, batch_size=64, shuffle=True)\n",
        "custom_loader = DataLoader(custom_data, batch_size=len(custom_data), shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlurZnBcGFrj"
      },
      "source": [
        "# create a method to find the max indices of the tensor outputs\n",
        "def batch_correct_num(data, values):\n",
        "  num_correct = 0\n",
        "  for x in range(len(values)):\n",
        "    if torch.argmax(data, dim=1)[x]==values[x].item():\n",
        "      num_correct+=1\n",
        "  return num_correct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OslPbbiMG8sT",
        "outputId": "e29f43f3-3789-4ebf-b05e-4c8fea2c32fd"
      },
      "source": [
        "# Now make the model as a class\n",
        "class Network(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Linear(784, 128), # input is the dimension of the image\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(128, 64),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 2) # will eventually be three outputs, 0 is 3, 1 is 7, 2 is neither\n",
        "    )\n",
        "    \n",
        "  def forward(self, data):\n",
        "    x = data.view(data.size(0), -1) # 64 x 784\n",
        "    output = self.layers(x)\n",
        "    return output\n",
        "\n",
        "n = Network()\n",
        "print(n)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network(\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=32, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9buQinHAr65",
        "outputId": "2be695df-5681-42de-9271-fd7fe84d3063"
      },
      "source": [
        "# Pass training data into the model\n",
        "optimizer = optim.Adam(n.parameters(), lr=1e-5) # learning rate is hyperparameter that should be played with\n",
        "EPOCHS = 10 # each epoch is a pass through all the data\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "loss_graphing = []\n",
        "accuracy_graphing = []\n",
        "test = False\n",
        "epochs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "for epoch in range(EPOCHS):\n",
        "  epoch_loss = 0\n",
        "  num_correct = 0\n",
        "  for data_X, data_y in training_loader:\n",
        "    optimizer.zero_grad()\n",
        "    output = n(data_X)\n",
        "    loss = loss_func(output, data_y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(loss)\n",
        "    epoch_loss += loss.item()\n",
        "    num_correct += batch_correct_num(output, data_y)\n",
        "    if test:\n",
        "      print(output.shape)\n",
        "      print(data_y)\n",
        "      print(data_y.shape)\n",
        "      print(output)\n",
        "      print(len(output))\n",
        "      print(predictions)\n",
        "      print(len(predictions))\n",
        "      test = False\n",
        "    \n",
        "  print(\"Epoch {}: Training Loss: {:.5f} Accuracy: {}/{}\".format(epoch+1, epoch_loss, num_correct, len(training_data)))\n",
        "  loss_graphing.append(epoch_loss)\n",
        "  accuracy_graphing.append(num_correct)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.7022, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7077, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6920, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6903, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6789, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6817, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6903, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6974, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7026, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6952, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7005, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6902, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6969, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6997, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6965, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7014, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6936, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7030, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7033, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6809, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6909, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6910, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6910, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6926, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6878, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6957, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6940, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6995, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7004, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6917, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6839, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6920, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7034, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6906, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6905, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6937, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6823, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6863, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6804, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6870, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6913, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6956, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6898, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6965, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7062, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6894, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6955, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6892, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6960, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6878, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6900, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6840, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6925, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6805, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6843, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6860, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6925, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6888, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6954, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6855, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6885, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6941, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6917, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6838, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6966, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7015, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6877, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6967, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6892, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6859, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6742, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6841, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6875, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7000, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6882, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6833, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6940, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6842, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6838, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6876, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6872, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6935, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6904, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6918, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6886, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6953, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6859, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6800, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6921, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6909, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6802, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6852, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6829, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6879, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6868, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6840, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6822, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6860, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6810, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6721, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6977, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6871, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6931, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6779, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6835, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6872, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6915, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6846, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6967, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6884, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6979, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6691, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6912, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6900, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6903, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6941, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6989, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6888, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6750, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6865, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6802, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6831, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6742, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6821, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6854, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6791, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6717, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6950, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6817, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6913, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6812, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6855, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6902, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6778, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6809, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6791, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6788, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6853, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6832, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6881, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6827, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6838, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6890, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6844, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6768, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6901, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6934, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6681, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6713, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6634, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6872, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6825, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6876, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6850, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6620, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6932, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6813, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6911, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6792, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6816, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6821, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6829, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6902, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6782, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6725, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6736, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6760, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6691, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6679, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6764, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6765, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6890, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6761, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6886, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6825, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6696, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6812, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6646, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6932, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6649, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6803, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6764, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6720, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6693, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6678, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6801, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6713, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6825, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6774, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6735, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6757, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6840, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6844, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6774, grad_fn=<NllLossBackward>)\n",
            "Epoch 1: Training Loss: 133.13610 Accuracy: 6265/12396\n",
            "tensor(0.6744, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6705, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6850, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6714, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6854, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6679, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6704, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6739, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6814, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6719, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6857, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6776, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6868, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6758, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6814, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6688, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6800, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6835, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6783, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6738, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6712, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6772, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6614, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6591, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6760, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6808, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6689, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6758, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6746, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6724, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6663, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6682, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6788, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6651, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6812, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6795, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6680, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6695, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6676, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6804, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6590, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6749, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6750, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6773, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6763, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6588, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6766, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6607, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6646, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6538, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6664, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6723, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6731, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6615, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6753, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6660, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6783, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6669, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6779, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6740, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6602, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6667, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6665, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6691, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6549, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6583, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6574, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6660, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6758, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6622, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6751, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6616, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6630, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6580, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6671, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6567, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6660, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6613, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6679, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6582, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6642, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6575, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6680, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6614, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6604, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6570, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6577, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6695, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6563, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6536, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6584, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6511, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6516, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6591, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6555, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6558, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6559, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6540, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6496, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6592, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6483, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6541, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6545, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6485, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6526, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6538, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6576, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6619, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6567, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6391, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6463, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6530, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6509, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6566, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6583, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6485, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6571, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6507, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6503, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6490, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6422, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6322, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6452, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6453, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6474, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6406, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6446, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6466, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6462, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6443, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6521, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6550, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6398, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6467, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6461, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6410, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6392, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6385, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6434, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6506, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6447, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6399, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6458, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6312, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6347, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6375, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6399, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6498, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6403, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6425, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6396, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6373, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6403, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6347, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6384, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6261, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6381, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6291, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6350, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6382, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6399, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6287, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6345, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6267, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6348, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6410, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6354, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6323, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6368, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6342, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6352, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6279, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6363, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6289, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6287, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6318, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6326, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6289, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6244, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6261, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6218, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6257, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6269, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6229, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6335, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6289, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6185, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6229, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6237, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6248, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6196, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6254, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6284, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6176, grad_fn=<NllLossBackward>)\n",
            "Epoch 2: Training Loss: 126.88594 Accuracy: 9006/12396\n",
            "tensor(0.6215, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6222, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6379, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6258, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6172, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6126, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6254, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6128, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6193, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6148, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6199, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6216, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6155, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6146, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6134, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6145, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6155, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6168, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6159, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6174, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6060, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6150, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6140, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6205, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6123, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6106, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6128, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6087, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6115, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6061, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6070, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6143, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6033, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6165, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6004, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6081, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6113, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6051, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5939, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6020, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6056, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6026, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6058, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6069, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6078, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5950, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6086, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6042, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6029, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5970, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6021, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5934, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6006, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5889, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5968, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6047, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5981, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6006, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5885, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5964, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5874, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5877, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5934, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5913, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5952, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5858, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5908, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5932, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5952, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5999, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5958, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5956, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5940, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5875, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5908, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5807, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5987, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5885, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5828, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5775, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5965, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5801, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5860, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5805, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5764, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5806, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5822, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5816, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5793, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5882, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5800, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5831, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5761, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5746, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5776, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5865, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5744, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5704, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5660, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5779, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5792, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5827, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5694, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5719, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5661, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5660, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5796, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5761, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5737, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5613, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5583, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5657, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5725, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5722, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5685, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5597, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5624, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5700, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5652, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5667, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5623, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5584, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5606, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5566, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5597, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5604, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5511, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5465, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5607, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5644, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5644, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5440, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5511, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5619, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5564, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5506, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5665, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5439, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5642, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5707, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5504, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5526, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5552, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5657, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5402, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5358, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5480, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5463, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5498, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5421, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5498, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5350, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5515, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5442, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5336, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5495, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5435, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5391, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5526, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5400, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5362, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5450, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5429, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5373, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5474, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5281, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5304, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5371, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5285, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5352, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5406, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5330, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5406, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5250, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5410, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5444, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5479, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5305, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5254, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5273, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5151, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5396, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5214, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5234, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5178, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5286, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5247, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5390, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5153, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5309, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5149, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5083, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5178, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5154, grad_fn=<NllLossBackward>)\n",
            "Epoch 3: Training Loss: 111.47325 Accuracy: 12045/12396\n",
            "tensor(0.5243, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5152, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5128, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5137, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5242, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5105, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5144, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5227, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5150, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5117, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5261, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5137, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5281, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5002, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5150, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5006, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5299, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5063, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4917, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5175, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5118, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5295, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4933, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5043, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5125, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5021, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5134, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5075, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4945, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4966, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4852, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4952, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4946, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5004, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5207, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4875, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5096, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4909, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4779, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4956, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4962, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4802, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4923, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4830, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4949, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4835, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4829, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4827, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4870, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4863, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4760, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4821, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5012, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4827, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5003, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4911, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4948, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4805, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4942, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4727, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4759, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4720, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4945, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4729, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4802, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4581, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4585, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4832, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4762, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4722, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4707, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4589, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4603, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4770, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4775, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4595, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4626, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4420, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4763, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4887, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4651, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4805, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4710, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4734, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4690, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4798, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4525, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4799, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4767, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4852, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4593, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4697, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4582, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4533, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4471, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4470, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4642, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4445, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4433, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4601, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4466, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4541, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4653, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4508, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4418, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4743, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4536, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4511, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4172, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4627, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4381, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4652, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4435, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4504, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4395, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4524, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4334, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4597, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4471, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4327, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4614, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4530, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4456, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4558, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4319, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4381, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4424, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4535, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4203, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4258, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4563, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4288, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4395, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4377, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4640, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4506, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4354, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4378, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4291, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4471, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4378, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4273, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4359, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4490, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4226, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4168, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4416, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4163, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4249, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4515, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4413, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4040, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4282, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4292, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4205, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4573, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4019, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4261, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4168, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4298, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4405, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4248, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4186, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4056, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4154, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4227, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4255, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4034, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4189, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4252, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4158, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4457, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4050, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4134, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4214, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3875, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4056, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4165, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3932, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4200, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3933, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3906, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4068, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4093, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4087, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4306, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4190, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3918, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4076, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3684, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4232, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3763, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4223, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4057, grad_fn=<NllLossBackward>)\n",
            "Epoch 4: Training Loss: 89.14936 Accuracy: 12099/12396\n",
            "tensor(0.4125, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3789, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3868, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4171, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3644, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3884, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4171, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3837, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3749, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3600, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4135, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4252, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4002, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4021, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3758, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4033, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3846, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3857, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4069, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4020, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3760, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3882, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3900, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4023, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3576, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3718, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3973, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3643, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3910, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3934, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3738, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3957, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3836, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3976, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3725, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4082, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3740, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3679, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4015, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4065, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3742, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3728, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3774, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3777, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3806, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3762, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3744, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3808, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4014, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3703, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3921, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3802, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3657, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3840, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3757, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3589, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3527, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3223, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3808, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3599, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3587, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3731, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3617, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3704, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3584, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3619, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3787, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3809, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3558, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3587, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3824, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3450, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3719, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3346, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3639, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3633, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3558, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3458, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3431, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3538, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3008, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3572, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3672, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3635, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3326, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3271, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3270, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3307, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3537, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3466, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3387, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4017, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3530, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3898, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3629, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3330, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3480, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3392, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3601, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3504, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3204, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3542, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3224, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3333, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3351, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3295, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3523, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3473, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3561, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4054, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3327, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3289, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3190, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3195, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3544, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3571, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3525, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3495, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3638, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3589, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3184, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3423, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3565, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3508, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3567, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3305, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3590, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3374, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3418, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3285, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3352, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3278, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3207, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3471, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3288, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3200, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3089, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2954, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3068, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3239, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3708, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3121, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3202, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3175, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2998, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3036, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3244, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3164, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3205, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3168, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3072, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3061, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3152, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3107, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2966, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3189, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3100, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3128, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2854, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2864, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3219, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2807, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2948, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2913, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3001, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3243, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3187, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3170, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3370, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3379, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3071, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3097, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3108, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3081, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3093, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2992, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2983, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2826, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3137, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2927, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3091, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3145, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3134, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3024, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3197, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2993, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2798, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2871, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3096, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3099, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2965, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2938, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2718, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3009, grad_fn=<NllLossBackward>)\n",
            "Epoch 5: Training Loss: 67.39875 Accuracy: 12114/12396\n",
            "tensor(0.3076, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3146, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2682, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2868, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2924, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3371, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2979, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2741, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2754, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3109, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3006, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2997, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3349, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3031, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2597, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2590, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2955, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2758, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2792, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2820, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2629, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2791, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2798, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2516, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2966, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3221, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3260, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2632, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2796, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2857, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2959, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2812, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2573, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2884, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3270, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2898, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2784, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3080, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2914, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2875, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2471, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2547, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2849, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2710, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2865, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2643, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2779, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2706, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2411, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3085, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2473, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2646, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2648, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2601, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2527, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2707, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2575, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2230, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2756, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2430, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2664, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3178, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2851, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2695, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2766, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2616, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2514, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2405, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2512, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2937, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2495, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2704, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2471, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2534, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2737, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2544, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2658, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2454, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2782, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2328, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2860, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2841, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2394, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2420, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2489, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2463, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2742, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2519, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2272, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2448, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2661, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2380, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2484, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2544, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2641, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2369, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2378, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2264, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2213, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2663, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2549, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2570, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2619, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2763, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2228, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2498, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2489, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2568, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2512, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2370, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2517, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2415, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2413, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2322, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2213, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2472, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2744, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2390, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2258, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2788, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2495, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2754, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2289, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2498, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2533, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1917, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2264, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2544, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2484, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2020, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2068, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2296, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1997, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2336, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2461, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2105, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2276, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2350, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2348, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2239, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2412, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2624, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2011, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2053, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2786, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2493, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2258, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2235, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2186, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2403, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2450, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2070, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2271, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2019, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2438, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2156, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2264, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2146, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2336, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2215, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2229, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2440, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2183, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2238, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2045, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2226, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2367, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2360, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1909, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2180, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2082, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2098, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2130, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1979, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2464, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2395, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2247, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2439, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2137, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2094, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1957, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2248, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1992, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2860, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1813, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2065, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2265, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1979, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1973, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1812, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2202, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2371, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2077, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2277, grad_fn=<NllLossBackward>)\n",
            "Epoch 6: Training Loss: 48.75360 Accuracy: 12126/12396\n",
            "tensor(0.2029, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2455, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2140, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1900, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2164, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1943, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1826, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2378, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2218, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1887, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2678, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1798, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1680, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1682, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1768, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2373, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2090, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1793, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1799, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1829, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2228, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2212, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2395, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2126, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2316, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1978, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1909, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1832, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1584, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1884, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2320, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2283, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2021, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2796, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1681, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2010, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1923, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2168, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1745, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2138, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1930, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2024, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1829, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1982, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2012, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1775, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2042, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2083, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1960, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1736, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1658, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2258, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2337, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1893, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1727, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2057, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1988, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1898, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2157, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2149, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2169, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1951, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1793, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1973, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2052, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2304, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1664, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1858, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1833, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2005, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2310, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1869, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1806, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1689, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1829, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2222, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2218, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1907, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1873, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1861, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2172, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2208, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1490, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1895, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1670, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1915, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1724, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1557, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1659, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1508, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1789, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1725, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1582, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2211, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1689, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1721, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1684, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1858, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1630, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1486, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1867, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1774, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1880, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1527, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1994, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1844, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1648, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1832, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1235, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1703, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1726, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1915, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1535, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1997, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1815, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1646, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2010, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1733, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1477, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1850, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2420, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1847, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1660, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1729, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1555, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1529, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2001, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2038, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1608, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1796, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1942, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2032, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1655, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1497, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1959, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1249, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1764, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1665, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2026, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1537, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1199, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1532, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2522, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1707, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1769, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1617, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1410, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1849, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1666, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2172, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1539, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1261, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1676, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1591, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1764, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1549, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1354, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1508, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2040, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1522, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1507, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1716, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1996, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1515, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1341, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1541, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1867, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1557, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1481, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1466, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1196, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1557, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1451, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1245, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1774, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1340, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1845, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1611, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1457, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1802, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1402, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1233, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1765, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1743, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1691, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1560, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1369, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1443, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1598, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1906, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1344, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1200, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1909, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1638, grad_fn=<NllLossBackward>)\n",
            "Epoch 7: Training Loss: 35.26575 Accuracy: 12139/12396\n",
            "tensor(0.1655, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1206, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1757, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1382, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2110, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1352, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1664, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1765, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1673, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1363, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1745, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1431, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1584, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1866, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1193, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1701, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1372, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1434, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1329, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1407, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1592, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2133, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1592, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1514, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1599, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1601, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1431, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1614, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1725, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1228, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1435, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1349, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1648, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1527, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1345, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1606, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2067, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1420, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1152, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1209, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1333, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1535, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1227, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1253, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1147, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1441, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2064, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1492, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1785, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1564, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1584, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1403, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1405, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1569, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1628, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1765, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1472, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1356, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1459, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1093, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1322, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1542, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1265, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1127, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1045, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1508, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1560, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1783, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1417, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1283, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1121, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1450, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1387, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1343, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1242, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1273, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1350, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1428, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1477, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1313, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1305, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1027, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1319, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1511, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1281, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1362, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1437, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1262, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1308, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1801, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1243, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1217, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1247, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1547, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1447, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1271, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1248, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1705, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1432, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0930, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1149, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1332, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1006, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2026, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1295, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1294, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1553, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0969, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1555, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1514, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1297, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1325, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1175, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1037, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1135, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1031, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1626, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0960, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1265, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1672, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1107, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1040, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1257, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1344, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2074, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1245, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1283, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1369, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1622, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1489, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1574, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1833, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1388, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1571, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1148, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1746, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1049, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1880, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1160, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1082, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1411, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1009, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1109, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1204, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1081, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1375, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1312, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1296, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1432, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1008, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0964, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1225, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1473, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1327, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0916, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2358, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1170, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1165, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1010, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1534, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1525, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1073, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1693, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1372, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0916, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1348, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1080, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1970, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0794, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1356, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1292, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1589, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1056, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1486, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0932, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1089, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1281, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0963, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1718, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1102, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1111, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0973, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1027, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0993, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1008, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1200, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1018, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1551, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1397, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1123, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1272, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1383, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1148, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0818, grad_fn=<NllLossBackward>)\n",
            "Epoch 8: Training Loss: 26.70018 Accuracy: 12149/12396\n",
            "tensor(0.1151, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1917, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1229, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1341, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0995, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1288, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1042, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0705, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1119, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1076, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0869, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0979, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1092, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0917, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1193, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1055, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1346, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0823, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1158, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0981, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0729, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1105, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1490, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1160, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1622, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1269, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1378, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1094, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1049, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1030, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1356, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1611, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0844, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1070, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1172, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0754, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1043, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1542, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1510, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1215, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1154, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1036, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1293, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0888, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0911, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1278, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0825, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1488, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1147, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0838, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0839, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0882, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1475, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0725, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1163, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1287, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1028, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1127, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1687, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1151, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1695, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1060, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1483, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1015, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0937, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0758, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1219, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0986, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1371, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0781, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1071, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1078, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0728, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1179, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1313, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1023, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1399, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0762, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0841, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1544, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1089, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1043, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1121, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1398, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0962, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0976, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1715, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1559, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1278, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0720, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1425, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0827, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0941, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1158, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0943, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1007, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0914, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0903, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1264, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0994, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1257, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1633, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1125, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1016, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1206, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1271, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1324, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1181, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1319, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0943, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1532, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0843, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0925, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1053, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1115, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1234, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0974, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0776, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0701, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0813, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1145, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0796, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0645, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1588, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1242, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0734, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0714, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1175, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1593, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1311, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0979, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0583, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0748, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0914, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0759, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1146, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1655, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0895, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0986, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1689, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1300, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1377, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0965, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0973, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1341, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1338, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0722, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1061, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1268, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0874, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1416, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1296, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0778, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0753, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1141, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1779, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0887, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1625, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1305, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0999, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0973, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1065, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1237, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1114, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1009, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0801, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0730, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1235, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0834, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1312, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1037, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0798, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1159, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0986, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0755, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0685, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1042, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0996, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1269, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1182, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0804, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0774, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0947, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0767, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0857, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0769, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1424, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0846, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1294, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1092, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1032, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0879, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1146, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0930, grad_fn=<NllLossBackward>)\n",
            "Epoch 9: Training Loss: 21.35344 Accuracy: 12157/12396\n",
            "tensor(0.0570, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1120, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0888, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1324, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1341, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0753, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0764, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1401, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1219, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0750, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0765, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0826, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1128, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1329, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1103, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0862, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0728, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1323, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1318, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0871, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0892, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0708, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1130, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0761, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0910, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0643, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0720, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0881, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0523, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1213, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0952, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0860, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0897, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1308, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0598, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1073, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0786, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0791, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0733, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0856, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1317, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1075, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1096, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1153, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1090, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1188, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0762, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1095, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1009, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0878, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1378, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1086, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0808, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0853, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1350, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0871, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0619, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0665, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1139, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0956, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0871, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1075, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1124, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1075, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0657, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0868, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0758, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0893, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0761, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0715, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1106, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0970, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0641, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0967, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1108, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1499, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0671, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0887, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0851, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1301, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0873, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0800, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0650, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0942, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0730, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1232, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0704, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1106, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0513, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0825, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0806, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0703, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0939, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0725, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1876, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1028, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0742, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1498, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0773, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0717, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0443, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0702, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0964, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1048, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0756, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0948, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1073, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0908, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0837, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0559, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1382, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1089, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0558, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0456, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1445, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0696, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0718, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0867, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1011, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0615, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0685, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0969, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0658, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0857, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0675, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1067, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1163, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1067, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0868, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0541, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0936, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1521, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0613, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0837, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0906, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0815, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0640, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0641, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0843, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1033, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1118, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0545, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0792, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0990, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1008, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0748, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0674, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1143, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1167, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0599, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0569, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0931, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0898, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0909, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0792, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1551, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1109, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1121, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0606, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0548, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0846, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0897, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0966, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0746, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0996, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0903, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1265, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0759, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1026, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0399, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0997, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0923, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1198, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0568, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0545, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2292, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0566, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0448, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0774, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1038, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1214, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0839, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1200, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0709, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1317, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1155, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0620, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1323, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0953, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1213, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1014, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0906, grad_fn=<NllLossBackward>)\n",
            "tensor(0.0653, grad_fn=<NllLossBackward>)\n",
            "tensor(0.1017, grad_fn=<NllLossBackward>)\n",
            "Epoch 10: Training Loss: 17.94998 Accuracy: 12168/12396\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "lHsg93C5RIhk",
        "outputId": "e32a0187-6b6c-4e5f-ec0d-c7dbc276b21e"
      },
      "source": [
        "# try the model on the cutom data\n",
        "fig=plt.figure()\n",
        "num_correct=0\n",
        "for data_X, data_y in custom_loader:\n",
        "  output = n(data_X.float())\n",
        "  num_correct += batch_correct_num(output, data_y)\n",
        "  for x in range(len(output)):\n",
        "    print(output[x])\n",
        "    fig.add_subplot(2, 2, x+1)\n",
        "    plt.imshow(myImages[x][0][0])\n",
        "    plt.axis('off')\n",
        "print('Accuracy: {}/{}'.format(num_correct, len(custom_data)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 2.1582, -3.0992], grad_fn=<SelectBackward>)\n",
            "tensor([ 1.4554, -2.0084], grad_fn=<SelectBackward>)\n",
            "Accuracy: 1/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAScAAABxCAYAAACX+mUfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAACzElEQVR4nO3dwY3TUBSF4WREFVRBE4gKqJIKEE1QBWXgWbCxAngSiPP+53zfNqPIi5eje24cz3lZlhNAzcvoCwD4E+EEJAknIEk4AUnCCUgSTkDSu60XP758dp9BxLefX86jr+FInO2Ov51tkxOQJJyAJOEEJAknIEk4AUnCCUgSTkCScAKShBOQJJyApM2frxzB1x/fr/7bT+8/7HglwC1MTkCScAKSpq11t9S1e7ynysdb9jiTNY/8HJicgCThBCQJJyApvXMqdfjLa7GDgn2ZnIAk4QQkDa91e1e3W+pXqUYyn1mrfvXcm5yAJOEEJAknIOnhO6d79ds9+v3le25d6/q1WXcNPK9rP4cjz7bJCUgSTkDSw2vdVnVSj2AfMz500eQEJAknIEk4AUnDf75S6bdwNDPcLrDF5AQkCScgaXitm1V1FIZrzHB+TU5AknACkoQTkGTntFJ9IiBc42jn1+QEJAknIOnpa93sd9HyvGZ80sAtTE5AknACkoQTkDTtzuloX5vCNY6+Z1ozOQFJwglISte6UnW7vJbZR2bm8Ew17pLJCUgSTkCScAKS0jundYcu7Z9OJ/8MlP34SdUvJicgSTgBScIJSErvnNZG9OvanotjeuZ7mbaYnIAk4QQkTVPrRijfysDc3C7wNpMTkCScgCThBCTZOcED2FnezuQEJAknIEmtg538a5V75tsH1kxOQJJwApKEE5A0fOe01ctHd29f//IIo895lckJSBJOQNLwWrelXKuM4lzy0Lj7MjkBScIJSBJOQNLwndMsT5u0I+B/OD+3MzkBScIJSBpe69buNfr6NTgjOD/3ZXICkoQTkCScgKTUzuledH+Yn8kJSBJOQJJwApKEE5AknIAk4QQkCScgSTgBScIJSBJOQJJwApKEE5AknICk87Iso68B4DcmJyBJOAFJwglIEk5AknACkoQTkPQK5ZeHmsDzc/gAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xB_PE_fuxmPZ",
        "outputId": "3677bd15-1d7d-45ff-9853-8eda41698101"
      },
      "source": [
        "# try the model on the testing data\n",
        "num_correct=0\n",
        "for data_X, data_y in testing_loader:\n",
        "  output = n(data_X.float())\n",
        "  num_correct += batch_correct_num(output, data_y)\n",
        "  print(output)\n",
        "print('Testing accuracy: {}/{}'.format(num_correct, len(testing_data)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-11.0073,  -3.8187],\n",
            "        [-10.6239,  -4.9528],\n",
            "        [-12.5406,  -2.7550],\n",
            "        [ -4.0037, -14.5596],\n",
            "        [ -4.2084, -13.8600],\n",
            "        [-10.1643,  -4.3996],\n",
            "        [-10.4995,  -4.2932],\n",
            "        [ -9.3628,  -5.7924],\n",
            "        [-12.6099,  -2.7596],\n",
            "        [ -4.1987, -13.8179],\n",
            "        [-10.3362,  -4.7342],\n",
            "        [ -4.0882, -14.1646],\n",
            "        [ -3.8805, -14.5642],\n",
            "        [ -3.6881, -15.0631],\n",
            "        [-10.7342,  -3.9824],\n",
            "        [-11.3041,  -3.8906],\n",
            "        [-10.5947,  -4.2073],\n",
            "        [ -1.7664, -19.3474],\n",
            "        [ -4.3053, -13.8104],\n",
            "        [-11.6859,  -3.4008],\n",
            "        [-10.1492,  -4.3488],\n",
            "        [-13.0135,  -2.3894],\n",
            "        [ -3.8831, -14.8068],\n",
            "        [ -4.7688, -14.5005],\n",
            "        [ -3.8230, -14.7516],\n",
            "        [ -2.6532, -17.2561],\n",
            "        [-11.1238,  -4.8480],\n",
            "        [ -5.5597, -11.0020],\n",
            "        [-12.7448,  -2.6010],\n",
            "        [-11.4354,  -3.5748],\n",
            "        [ -2.3892, -17.9057],\n",
            "        [-12.0575,  -3.0870],\n",
            "        [ -4.7313, -12.8156],\n",
            "        [-12.2557,  -3.0664],\n",
            "        [ -4.5279, -13.1535],\n",
            "        [ -4.5689, -13.0076],\n",
            "        [ -3.5569, -15.3565],\n",
            "        [-14.0890,  -1.8813],\n",
            "        [-11.1899,  -3.7720],\n",
            "        [ -4.2611, -13.9777],\n",
            "        [ -3.0390, -16.4247],\n",
            "        [-11.4767,  -3.4943],\n",
            "        [-11.4428,  -3.5235],\n",
            "        [ -4.4672, -13.2180],\n",
            "        [ -4.9729, -12.8457],\n",
            "        [ -2.4519, -17.6609],\n",
            "        [ -3.6350, -15.2904],\n",
            "        [ -9.7397,  -5.0140],\n",
            "        [-10.5750,  -4.0683],\n",
            "        [-11.1023,  -3.7129],\n",
            "        [ -4.9605, -12.1251],\n",
            "        [-12.6058,  -2.6849],\n",
            "        [-10.5945,  -5.7652],\n",
            "        [-11.2179,  -3.7351],\n",
            "        [ -3.5549, -15.3571],\n",
            "        [ -4.1356, -14.0123],\n",
            "        [ -5.3745, -11.2259],\n",
            "        [ -4.7817, -12.5797],\n",
            "        [-11.6534,  -3.4008],\n",
            "        [ -2.7324, -17.3588],\n",
            "        [ -4.2827, -13.7376],\n",
            "        [ -4.0361, -14.2334],\n",
            "        [ -2.8232, -17.0510],\n",
            "        [ -4.1647, -14.0763]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[-10.9824,  -4.3854],\n",
            "        [ -2.1812, -18.5080],\n",
            "        [-11.1493,  -4.2015],\n",
            "        [ -5.9798,  -9.9955],\n",
            "        [ -3.0318, -16.7116],\n",
            "        [-11.8250,  -4.3688],\n",
            "        [-10.6988,  -4.5567],\n",
            "        [ -5.0726, -12.2219],\n",
            "        [ -3.6508, -15.3890],\n",
            "        [-10.9995,  -4.2842],\n",
            "        [-10.9435,  -4.3327],\n",
            "        [-11.3027,  -4.0315],\n",
            "        [-12.5338,  -3.3392],\n",
            "        [ -4.3626, -13.7229],\n",
            "        [ -2.9395, -16.8785],\n",
            "        [-11.8205,  -3.6866],\n",
            "        [-10.0304,  -5.5095],\n",
            "        [-10.6656,  -4.7442],\n",
            "        [-11.4363,  -3.9479],\n",
            "        [ -4.6961, -12.9095],\n",
            "        [ -3.5151, -15.6209],\n",
            "        [-12.4625,  -3.2724],\n",
            "        [-15.0298,  -1.5864],\n",
            "        [ -4.6617, -12.9655],\n",
            "        [ -4.8012, -12.7999],\n",
            "        [-11.0473,  -4.2292],\n",
            "        [ -4.3824, -13.6894],\n",
            "        [ -5.2298, -11.7341],\n",
            "        [-11.1102,  -4.2851],\n",
            "        [-11.2485,  -4.0865],\n",
            "        [ -3.2866, -16.0584],\n",
            "        [-11.7890,  -3.7431],\n",
            "        [-13.3800,  -2.6426],\n",
            "        [-11.8592,  -3.6613],\n",
            "        [-13.5620,  -2.6403],\n",
            "        [ -4.5378, -13.3633],\n",
            "        [-13.8434,  -2.3721],\n",
            "        [-10.5595,  -5.0100],\n",
            "        [ -5.9362, -10.8488],\n",
            "        [-10.7680,  -4.8348],\n",
            "        [-11.8763,  -3.6612],\n",
            "        [ -4.4940, -13.3555],\n",
            "        [ -5.6558, -10.8347],\n",
            "        [-13.1501,  -2.8615],\n",
            "        [-11.4533,  -4.0211],\n",
            "        [ -3.3664, -15.9654],\n",
            "        [-11.6294,  -4.3478],\n",
            "        [-11.4348,  -4.0630],\n",
            "        [-12.3137,  -3.3469],\n",
            "        [ -9.4993,  -6.0274],\n",
            "        [ -4.6520, -13.0743],\n",
            "        [-11.2465,  -4.0894],\n",
            "        [ -3.9857, -14.5907],\n",
            "        [ -9.6962,  -5.9358],\n",
            "        [ -5.0434, -12.1852],\n",
            "        [-10.6482,  -4.6556],\n",
            "        [ -4.9775, -12.3933],\n",
            "        [ -9.2953,  -5.4545],\n",
            "        [ -5.1262, -12.1744],\n",
            "        [ -0.7602, -21.7499],\n",
            "        [ -4.0815, -14.5901],\n",
            "        [ -3.4535, -15.6921],\n",
            "        [-11.4451,  -3.9592],\n",
            "        [ -9.7536,  -5.5957]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[-11.9491,  -3.3564],\n",
            "        [ -3.7021, -15.1880],\n",
            "        [ -4.6818, -13.0388],\n",
            "        [ -2.9061, -16.9732],\n",
            "        [ -3.2008, -16.3765],\n",
            "        [-11.3334,  -3.9427],\n",
            "        [-13.6152,  -2.5119],\n",
            "        [-11.6183,  -3.5912],\n",
            "        [-12.6361,  -3.0716],\n",
            "        [ -4.5942, -13.2098],\n",
            "        [ -3.3430, -15.9316],\n",
            "        [-10.9852,  -4.0419],\n",
            "        [-10.9995,  -3.9605],\n",
            "        [ -4.3140, -13.7542],\n",
            "        [ -4.5592, -13.3372],\n",
            "        [ -4.6482, -13.0886],\n",
            "        [-12.1821,  -3.4876],\n",
            "        [ -9.6350,  -5.8822],\n",
            "        [ -3.2759, -16.1251],\n",
            "        [ -3.9118, -14.7654],\n",
            "        [ -5.3899, -11.3216],\n",
            "        [ -3.3010, -16.0972],\n",
            "        [-11.3244,  -4.9253],\n",
            "        [ -3.7297, -15.0837],\n",
            "        [-11.3361,  -3.7264],\n",
            "        [ -7.5596,  -8.1177],\n",
            "        [ -4.3370, -13.7343],\n",
            "        [ -4.2949, -13.9119],\n",
            "        [ -4.0713, -14.8061],\n",
            "        [-10.7492,  -4.1874],\n",
            "        [ -3.3277, -16.1053],\n",
            "        [-11.2883,  -3.7781],\n",
            "        [-12.4726,  -3.1638],\n",
            "        [ -2.7609, -17.2578],\n",
            "        [-13.4321,  -2.3407],\n",
            "        [ -3.8787, -15.0254],\n",
            "        [-12.0081,  -3.4602],\n",
            "        [-12.0673,  -3.2784],\n",
            "        [ -3.2041, -16.2897],\n",
            "        [ -4.3926, -13.5446],\n",
            "        [ -3.5091, -15.5118],\n",
            "        [-11.0579,  -4.1844],\n",
            "        [-13.4945,  -2.3146],\n",
            "        [ -3.6324, -15.3493],\n",
            "        [ -5.7656, -10.5277],\n",
            "        [ -4.4223, -13.6069],\n",
            "        [ -2.9242, -16.8661],\n",
            "        [ -3.4036, -16.1150],\n",
            "        [-11.9279,  -4.0300],\n",
            "        [-13.1349,  -2.6140],\n",
            "        [-11.6715,  -3.5529],\n",
            "        [ -4.0689, -14.3766],\n",
            "        [ -4.2152, -14.0270],\n",
            "        [-13.9614,  -2.0356],\n",
            "        [-12.0529,  -3.5433],\n",
            "        [ -3.7486, -15.0575],\n",
            "        [-11.4388,  -3.6659],\n",
            "        [ -9.6765,  -5.5100],\n",
            "        [ -3.9099, -14.7065],\n",
            "        [ -1.9863, -18.9920],\n",
            "        [ -3.3903, -16.0277],\n",
            "        [-10.5035,  -4.3236],\n",
            "        [-10.9777,  -4.9234],\n",
            "        [ -3.0718, -16.6133]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[-12.4228,  -2.6679],\n",
            "        [-11.3754,  -5.4357],\n",
            "        [-13.5405,  -2.3689],\n",
            "        [-10.1508,  -4.9206],\n",
            "        [-10.3319,  -4.5673],\n",
            "        [-11.6631,  -3.1992],\n",
            "        [ -3.8697, -14.1040],\n",
            "        [ -4.0693, -13.7142],\n",
            "        [ -4.5033, -12.6820],\n",
            "        [ -4.6263, -12.4496],\n",
            "        [-11.2814,  -3.4856],\n",
            "        [-11.2453,  -4.3956],\n",
            "        [-12.1589,  -3.0403],\n",
            "        [ -2.6385, -16.8679],\n",
            "        [ -3.0325, -16.0184],\n",
            "        [-10.9848,  -3.7929],\n",
            "        [ -3.1716, -15.7371],\n",
            "        [ -4.1405, -13.6190],\n",
            "        [ -5.1966, -11.2418],\n",
            "        [ -2.6595, -16.7893],\n",
            "        [ -5.1612, -11.8236],\n",
            "        [-11.6681,  -3.2960],\n",
            "        [ -4.2394, -13.4165],\n",
            "        [-10.4737,  -3.9964],\n",
            "        [-12.3897,  -2.7163],\n",
            "        [ -7.8026,  -7.5395],\n",
            "        [ -2.8593, -16.4308],\n",
            "        [-11.0412,  -3.7335],\n",
            "        [-10.9459,  -3.6705],\n",
            "        [-10.1354,  -4.5839],\n",
            "        [ -5.0377, -13.2307],\n",
            "        [ -2.7678, -16.6838],\n",
            "        [ -1.5650, -19.2815],\n",
            "        [-10.0985,  -4.6401],\n",
            "        [ -3.9843, -13.9448],\n",
            "        [ -4.1591, -13.5562],\n",
            "        [ -4.5660, -12.5390],\n",
            "        [ -5.3896, -10.8571],\n",
            "        [ -3.1925, -15.7445],\n",
            "        [-12.2475,  -2.8011],\n",
            "        [ -9.6884,  -5.8739],\n",
            "        [ -3.6487, -14.5875],\n",
            "        [-10.1523,  -4.7502],\n",
            "        [ -4.9102, -11.9090],\n",
            "        [-11.3166,  -3.4503],\n",
            "        [ -3.4623, -15.0263],\n",
            "        [ -5.3315, -11.1539],\n",
            "        [ -4.3974, -13.0157],\n",
            "        [ -4.3057, -13.1525],\n",
            "        [ -3.2790, -15.4158],\n",
            "        [-12.8597,  -2.6764],\n",
            "        [ -6.7788,  -8.8157],\n",
            "        [-11.9883,  -2.9335],\n",
            "        [ -3.5011, -14.9565],\n",
            "        [ -5.0302, -11.8293],\n",
            "        [ -5.3564, -11.2627],\n",
            "        [ -8.2306,  -6.7783],\n",
            "        [-12.5037,  -2.6517],\n",
            "        [-12.7510,  -2.4583],\n",
            "        [ -3.2070, -15.7372],\n",
            "        [ -4.5024, -12.8004],\n",
            "        [-11.4644,  -3.5945],\n",
            "        [ -4.2864, -13.2463],\n",
            "        [-12.2501,  -2.8368]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[-10.8207,  -4.0928],\n",
            "        [ -4.3924, -12.9506],\n",
            "        [ -2.8703, -16.3574],\n",
            "        [ -5.1036, -12.3472],\n",
            "        [ -4.6008, -12.5376],\n",
            "        [ -3.4537, -15.1771],\n",
            "        [-11.7454,  -3.6173],\n",
            "        [ -3.8145, -14.3323],\n",
            "        [-11.9424,  -3.5252],\n",
            "        [-10.5573,  -4.1009],\n",
            "        [ -3.0020, -16.0767],\n",
            "        [-12.0057,  -3.1808],\n",
            "        [-10.4857,  -4.3311],\n",
            "        [-10.6969,  -4.3391],\n",
            "        [-10.0783,  -4.5623],\n",
            "        [ -3.6470, -14.5775],\n",
            "        [ -3.5404, -14.8095],\n",
            "        [ -2.4432, -17.4587],\n",
            "        [ -2.0713, -18.0526],\n",
            "        [ -5.1478, -11.9616],\n",
            "        [-10.7569,  -4.0568],\n",
            "        [-12.9940,  -2.4952],\n",
            "        [ -3.9182, -13.9583],\n",
            "        [ -8.7055,  -6.4789],\n",
            "        [ -2.8504, -16.4782],\n",
            "        [-10.5716,  -5.7818],\n",
            "        [ -4.3701, -13.1791],\n",
            "        [ -4.4273, -12.9966],\n",
            "        [-10.2341,  -4.8476],\n",
            "        [-14.1257,  -1.7647],\n",
            "        [-11.3616,  -3.5994],\n",
            "        [-13.0651,  -2.4623],\n",
            "        [-10.1575,  -4.6009],\n",
            "        [ -3.0924, -15.8469],\n",
            "        [ -9.3918,  -4.9480],\n",
            "        [ -3.2711, -15.5488],\n",
            "        [ -4.4407, -12.8985],\n",
            "        [-10.9181,  -4.3673],\n",
            "        [-12.2102,  -3.0156],\n",
            "        [ -4.6877, -12.5553],\n",
            "        [-12.0690,  -3.1298],\n",
            "        [-11.4456,  -3.4970],\n",
            "        [ -3.9709, -13.9215],\n",
            "        [ -2.9410, -16.2166],\n",
            "        [ -3.3621, -15.3097],\n",
            "        [ -9.7131,  -6.4362],\n",
            "        [-10.7659,  -4.3513],\n",
            "        [ -3.8252, -14.2935],\n",
            "        [-11.0763,  -3.7805],\n",
            "        [-12.3319,  -3.0081],\n",
            "        [ -3.9959, -13.9368],\n",
            "        [-11.1525,  -3.7609],\n",
            "        [ -4.0052, -13.7675],\n",
            "        [-10.7238,  -4.5050],\n",
            "        [ -3.8479, -14.2118],\n",
            "        [ -4.0269, -13.6979],\n",
            "        [-10.5053,  -4.9179],\n",
            "        [-11.0283,  -3.8572],\n",
            "        [ -4.4756, -12.8058],\n",
            "        [-10.6080,  -4.1353],\n",
            "        [ -2.3166, -17.5222],\n",
            "        [-11.1453,  -3.7767],\n",
            "        [-13.6287,  -2.3545],\n",
            "        [ -4.1152, -13.6254]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[-10.3242,  -4.2116],\n",
            "        [-12.9146,  -2.5216],\n",
            "        [ -4.6080, -12.2529],\n",
            "        [ -3.5574, -14.4457],\n",
            "        [ -3.4649, -14.9847],\n",
            "        [ -4.5583, -13.2156],\n",
            "        [ -5.7795, -10.9959],\n",
            "        [-10.5439,  -5.0534],\n",
            "        [-11.5384,  -3.3286],\n",
            "        [ -4.5826, -12.2431],\n",
            "        [ -6.0472,  -9.3975],\n",
            "        [-10.4626,  -4.8294],\n",
            "        [ -4.6024, -12.1746],\n",
            "        [ -2.3456, -17.2262],\n",
            "        [-11.7741,  -3.2202],\n",
            "        [ -3.7073, -14.1914],\n",
            "        [-11.9711,  -3.1761],\n",
            "        [-10.8209,  -3.8487],\n",
            "        [-11.0424,  -3.5981],\n",
            "        [ -4.0812, -13.4429],\n",
            "        [ -3.9038, -13.7066],\n",
            "        [-11.5772,  -3.2723],\n",
            "        [ -3.6815, -14.1870],\n",
            "        [ -2.0745, -17.8462],\n",
            "        [-10.6585,  -3.9631],\n",
            "        [ -3.8318, -13.9383],\n",
            "        [ -8.6735,  -6.3989],\n",
            "        [ -5.1129, -11.0850],\n",
            "        [-10.5472,  -3.9763],\n",
            "        [ -3.2114, -15.2348],\n",
            "        [ -3.3882, -14.7885],\n",
            "        [ -3.6760, -14.3101],\n",
            "        [ -2.7819, -16.3619],\n",
            "        [-11.3363,  -3.9825],\n",
            "        [ -5.1812, -12.2304],\n",
            "        [-11.2631,  -3.5245],\n",
            "        [ -5.2630, -10.9489],\n",
            "        [ -4.1923, -13.1350],\n",
            "        [ -3.9450, -13.6297],\n",
            "        [-11.7734,  -3.1745],\n",
            "        [-11.5270,  -3.4000],\n",
            "        [-11.4871,  -3.3845],\n",
            "        [ -6.7459,  -9.3605],\n",
            "        [-12.2639,  -2.9003],\n",
            "        [-14.1304,  -1.6329],\n",
            "        [ -4.1306, -13.3448],\n",
            "        [-10.5765,  -4.1657],\n",
            "        [-12.7967,  -2.4650],\n",
            "        [ -3.6149, -14.4271],\n",
            "        [-10.5999,  -3.9411],\n",
            "        [ -3.5053, -14.6246],\n",
            "        [-11.6425,  -3.3231],\n",
            "        [-11.5577,  -3.3269],\n",
            "        [ -4.8109, -11.7199],\n",
            "        [ -4.4217, -12.6928],\n",
            "        [ -2.3229, -17.2153],\n",
            "        [-11.4404,  -3.8210],\n",
            "        [ -4.4803, -12.4056],\n",
            "        [ -3.8981, -13.7435],\n",
            "        [-11.3713,  -3.6791],\n",
            "        [ -3.2209, -15.3838],\n",
            "        [ -5.6934, -11.4840],\n",
            "        [ -5.1454, -11.0654],\n",
            "        [ -2.5512, -16.7630]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[ -2.0337, -18.0583],\n",
            "        [ -9.1326,  -6.2309],\n",
            "        [ -4.0992, -13.5012],\n",
            "        [ -3.4289, -15.0549],\n",
            "        [ -2.3924, -17.3161],\n",
            "        [ -9.6157,  -4.9976],\n",
            "        [-12.6623,  -2.5646],\n",
            "        [ -3.6896, -14.4624],\n",
            "        [-12.1045,  -3.1583],\n",
            "        [ -4.8679, -11.8562],\n",
            "        [ -4.1713, -13.3244],\n",
            "        [-12.7178,  -2.5114],\n",
            "        [-11.1936,  -3.5774],\n",
            "        [-10.3045,  -4.4887],\n",
            "        [-10.7024,  -3.8912],\n",
            "        [-11.8029,  -3.1263],\n",
            "        [-11.1484,  -3.5845],\n",
            "        [ -2.5905, -16.7994],\n",
            "        [ -3.5803, -14.7264],\n",
            "        [-11.9297,  -3.5645],\n",
            "        [ -9.6005,  -6.0775],\n",
            "        [-11.0304,  -4.5664],\n",
            "        [-13.8347,  -1.7995],\n",
            "        [ -9.6465,  -5.0251],\n",
            "        [-10.6668,  -3.8803],\n",
            "        [ -9.7855,  -4.8627],\n",
            "        [-10.9838,  -3.7373],\n",
            "        [ -5.1349, -12.4233],\n",
            "        [-12.8655,  -2.4324],\n",
            "        [ -3.0101, -15.8736],\n",
            "        [ -8.2527,  -6.7364],\n",
            "        [ -6.1121,  -8.8911],\n",
            "        [-11.5079,  -3.8657],\n",
            "        [ -1.7050, -18.8022],\n",
            "        [-11.3161,  -3.9178],\n",
            "        [ -4.6755, -12.1289],\n",
            "        [ -3.5900, -14.6916],\n",
            "        [ -5.0201, -11.4310],\n",
            "        [ -8.4736,  -6.9224],\n",
            "        [ -3.6911, -14.4059],\n",
            "        [ -5.0737, -11.2980],\n",
            "        [-10.0187,  -5.7851],\n",
            "        [ -4.6512, -12.1017],\n",
            "        [ -2.0436, -18.2048],\n",
            "        [-10.3611,  -4.1182],\n",
            "        [ -3.2917, -15.3004],\n",
            "        [-10.9499,  -3.7194],\n",
            "        [-10.6427,  -4.1999],\n",
            "        [-11.7709,  -3.1526],\n",
            "        [ -9.7797,  -5.0672],\n",
            "        [ -4.1905, -13.3089],\n",
            "        [-10.0514,  -4.4192],\n",
            "        [ -4.8951, -11.6488],\n",
            "        [ -4.4716, -12.6336],\n",
            "        [-12.0975,  -3.0281],\n",
            "        [ -5.1854, -11.3806],\n",
            "        [ -4.2363, -13.2010],\n",
            "        [-12.2964,  -3.2048],\n",
            "        [-10.1796,  -4.5117],\n",
            "        [-11.1105,  -4.2900],\n",
            "        [ -4.9151, -11.6597],\n",
            "        [ -3.6982, -14.3541],\n",
            "        [-11.4590,  -3.6618],\n",
            "        [ -9.9634,  -5.4684]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[-13.2219,  -2.6330],\n",
            "        [ -4.0593, -14.9512],\n",
            "        [ -4.9984, -12.3456],\n",
            "        [ -3.7196, -15.2754],\n",
            "        [-13.8838,  -2.1983],\n",
            "        [-12.1203,  -3.3368],\n",
            "        [-12.5566,  -3.9097],\n",
            "        [-14.6817,  -1.9371],\n",
            "        [ -5.7793, -10.6816],\n",
            "        [-11.3580,  -3.9581],\n",
            "        [-12.3968,  -3.2574],\n",
            "        [ -3.2503, -16.2608],\n",
            "        [-11.2796,  -3.9344],\n",
            "        [-11.8208,  -3.6457],\n",
            "        [-11.1041,  -4.3411],\n",
            "        [-11.0526,  -4.2097],\n",
            "        [ -4.8861, -13.6550],\n",
            "        [-11.7205,  -3.9892],\n",
            "        [ -3.5800, -15.5015],\n",
            "        [ -4.8282, -13.0491],\n",
            "        [-12.4845,  -3.1194],\n",
            "        [ -4.3978, -13.7168],\n",
            "        [ -3.1013, -17.1883],\n",
            "        [ -5.2166, -11.8855],\n",
            "        [-10.6290,  -4.3612],\n",
            "        [-11.3426,  -4.1758],\n",
            "        [ -5.1072, -12.0352],\n",
            "        [ -4.8436, -12.7318],\n",
            "        [ -3.7529, -15.2608],\n",
            "        [ -4.0131, -14.5498],\n",
            "        [-10.3799,  -5.3345],\n",
            "        [-11.1543,  -4.0736],\n",
            "        [-11.8490,  -3.6088],\n",
            "        [ -3.5755, -15.6018],\n",
            "        [-11.5024,  -3.7801],\n",
            "        [ -1.5447, -20.1575],\n",
            "        [-10.2025,  -5.0701],\n",
            "        [ -3.6691, -15.5501],\n",
            "        [-10.4904,  -4.4147],\n",
            "        [-11.4966,  -3.8420],\n",
            "        [ -4.1021, -14.5080],\n",
            "        [ -8.9688,  -5.7579],\n",
            "        [ -8.2921,  -6.6266],\n",
            "        [ -2.6441, -17.7793],\n",
            "        [-12.4674,  -3.1644],\n",
            "        [-12.6798,  -3.0828],\n",
            "        [-12.0349,  -4.3822],\n",
            "        [-10.4527,  -5.4556],\n",
            "        [-10.8766,  -4.2450],\n",
            "        [-10.8419,  -4.3453],\n",
            "        [-12.1645,  -3.3240],\n",
            "        [-12.4227,  -3.1604],\n",
            "        [ -2.5508, -17.8048],\n",
            "        [-10.9625,  -4.5492],\n",
            "        [ -3.6334, -15.3130],\n",
            "        [ -4.6393, -13.1658],\n",
            "        [ -3.1017, -16.5758],\n",
            "        [-13.2059,  -3.5142],\n",
            "        [-11.4458,  -3.8788],\n",
            "        [ -3.8394, -15.0961],\n",
            "        [ -2.5403, -17.9896],\n",
            "        [-10.8480,  -4.8550],\n",
            "        [ -2.0955, -18.8236],\n",
            "        [ -4.5473, -13.3965]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[ -5.8505, -10.8251],\n",
            "        [-13.0711,  -2.5211],\n",
            "        [-11.9339,  -3.3009],\n",
            "        [-11.8956,  -3.3581],\n",
            "        [-11.8770,  -3.3656],\n",
            "        [-10.1420,  -4.8253],\n",
            "        [ -9.6828,  -4.9461],\n",
            "        [ -4.3263, -13.2911],\n",
            "        [-10.6813,  -4.2761],\n",
            "        [-11.6536,  -3.5652],\n",
            "        [-12.1944,  -3.1316],\n",
            "        [-11.2325,  -3.7596],\n",
            "        [ -1.7110, -19.2595],\n",
            "        [-11.0519,  -3.9189],\n",
            "        [-10.0557,  -4.6317],\n",
            "        [-11.4487,  -3.6586],\n",
            "        [ -3.4988, -15.3067],\n",
            "        [-10.3815,  -4.3852],\n",
            "        [ -5.6267, -10.4494],\n",
            "        [ -3.9986, -14.1638],\n",
            "        [ -3.0731, -16.0789],\n",
            "        [-12.8676,  -2.6912],\n",
            "        [ -2.7405, -16.8069],\n",
            "        [-13.3312,  -2.5825],\n",
            "        [ -7.1158,  -7.9343],\n",
            "        [ -8.8656,  -5.6653],\n",
            "        [-10.2652,  -5.4952],\n",
            "        [ -3.2962, -15.6894],\n",
            "        [ -5.1686, -11.4231],\n",
            "        [-12.9006,  -2.6843],\n",
            "        [-10.4592,  -4.4403],\n",
            "        [ -3.0023, -16.4164],\n",
            "        [-11.7479,  -3.5826],\n",
            "        [-10.7168,  -4.2472],\n",
            "        [ -9.8651,  -4.9855],\n",
            "        [ -4.0661, -13.8448],\n",
            "        [-12.1680,  -3.1622],\n",
            "        [-11.0493,  -4.1207],\n",
            "        [ -1.5074, -19.7449],\n",
            "        [-10.5117,  -4.2709],\n",
            "        [ -4.5209, -12.9044],\n",
            "        [ -3.7993, -14.5105],\n",
            "        [-10.9263,  -4.4578],\n",
            "        [ -9.8443,  -5.3146],\n",
            "        [-11.6634,  -3.5055],\n",
            "        [ -4.2351, -13.4695],\n",
            "        [ -3.2506, -15.8998],\n",
            "        [-13.5303,  -2.2709],\n",
            "        [ -5.0959, -11.6168],\n",
            "        [ -3.3992, -15.4046],\n",
            "        [ -4.5476, -12.8963],\n",
            "        [ -3.6637, -14.8437],\n",
            "        [ -5.2812, -11.1766],\n",
            "        [ -3.3951, -15.4175],\n",
            "        [ -5.5256, -10.6826],\n",
            "        [ -5.4424, -10.7453],\n",
            "        [-10.1195,  -4.5599],\n",
            "        [-12.9857,  -2.6252],\n",
            "        [ -3.2341, -15.7296],\n",
            "        [-11.8972,  -3.6081],\n",
            "        [ -3.5826, -15.0163],\n",
            "        [ -4.0200, -14.0312],\n",
            "        [ -4.3237, -13.6143],\n",
            "        [ -9.8091,  -4.7557]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[-10.3047,  -4.5981],\n",
            "        [-11.0574,  -4.5916],\n",
            "        [-11.8858,  -3.7700],\n",
            "        [ -4.6793, -12.9590],\n",
            "        [ -3.2097, -16.3513],\n",
            "        [ -2.9525, -16.7282],\n",
            "        [ -3.5175, -15.3764],\n",
            "        [ -2.1981, -18.4226],\n",
            "        [ -3.2006, -16.1454],\n",
            "        [ -4.9854, -12.2466],\n",
            "        [-13.0329,  -2.6604],\n",
            "        [ -5.7569, -10.6895],\n",
            "        [ -4.2647, -13.8653],\n",
            "        [ -2.4900, -17.8391],\n",
            "        [ -4.1960, -14.0616],\n",
            "        [-11.9077,  -4.1810],\n",
            "        [-11.4060,  -3.8673],\n",
            "        [-11.3817,  -4.2016],\n",
            "        [-12.1853,  -3.3239],\n",
            "        [ -5.3095, -11.8586],\n",
            "        [-12.9868,  -2.7544],\n",
            "        [-14.0303,  -2.0337],\n",
            "        [ -5.8564, -11.4256],\n",
            "        [-12.5247,  -3.1944],\n",
            "        [ -4.2742, -13.8246],\n",
            "        [ -3.4720, -15.6525],\n",
            "        [-13.6630,  -2.2686],\n",
            "        [-12.1178,  -3.2989],\n",
            "        [-10.1224,  -4.8176],\n",
            "        [ -4.3238, -13.8069],\n",
            "        [-10.5605,  -4.5000],\n",
            "        [ -4.1388, -14.1392],\n",
            "        [-11.3885,  -3.8919],\n",
            "        [ -4.9178, -12.3637],\n",
            "        [-11.6613,  -3.7599],\n",
            "        [-14.4672,  -1.7519],\n",
            "        [ -4.4820, -13.8437],\n",
            "        [ -4.4794, -13.6546],\n",
            "        [ -3.8256, -15.1175],\n",
            "        [ -8.1187,  -7.5419],\n",
            "        [-11.3531,  -4.0936],\n",
            "        [ -5.7730, -10.3861],\n",
            "        [ -2.9835, -16.8502],\n",
            "        [-11.6789,  -3.5997],\n",
            "        [ -9.8330,  -5.2914],\n",
            "        [-10.0816,  -4.8322],\n",
            "        [ -5.0500, -12.0940],\n",
            "        [ -9.2520,  -5.6025],\n",
            "        [-10.4889,  -5.2829],\n",
            "        [ -4.3662, -13.5369],\n",
            "        [ -4.4090, -13.6372],\n",
            "        [-12.0725,  -3.3468],\n",
            "        [-11.0254,  -4.0922],\n",
            "        [-11.6850,  -3.5938],\n",
            "        [ -4.9380, -12.3572],\n",
            "        [ -1.7135, -19.5472],\n",
            "        [-11.5716,  -3.6999],\n",
            "        [-10.7838,  -5.2469],\n",
            "        [ -3.8402, -14.6930],\n",
            "        [ -3.3024, -15.9486],\n",
            "        [ -2.7262, -17.2810],\n",
            "        [ -2.9838, -16.7236],\n",
            "        [ -4.3151, -13.7768],\n",
            "        [-10.4801,  -4.4518]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[ -7.1431,  -8.1756],\n",
            "        [ -9.3041,  -5.6979],\n",
            "        [ -2.6293, -16.6450],\n",
            "        [ -5.2839, -10.9155],\n",
            "        [-12.0013,  -3.1078],\n",
            "        [-10.9984,  -4.0953],\n",
            "        [-10.6828,  -4.5230],\n",
            "        [ -9.0233,  -5.0853],\n",
            "        [ -2.8368, -16.0794],\n",
            "        [-12.7160,  -2.5209],\n",
            "        [ -3.6471, -14.1875],\n",
            "        [-11.0413,  -3.8820],\n",
            "        [-11.5424,  -3.3344],\n",
            "        [ -9.3398,  -5.1836],\n",
            "        [ -9.8690,  -5.5519],\n",
            "        [ -3.2788, -15.0462],\n",
            "        [ -7.1876,  -8.4122],\n",
            "        [ -3.5224, -14.4167],\n",
            "        [-12.7846,  -2.5805],\n",
            "        [-10.9240,  -3.7881],\n",
            "        [ -4.2537, -12.8929],\n",
            "        [ -2.9244, -15.8413],\n",
            "        [ -5.7797, -10.6596],\n",
            "        [ -4.3495, -12.7033],\n",
            "        [ -4.8766, -11.6802],\n",
            "        [ -3.7314, -13.9730],\n",
            "        [ -9.8147,  -4.9276],\n",
            "        [ -4.5063, -12.1760],\n",
            "        [ -2.8222, -16.1120],\n",
            "        [ -4.0130, -13.3456],\n",
            "        [ -4.2615, -12.8531],\n",
            "        [ -9.1493,  -5.2928],\n",
            "        [ -5.9218, -10.9518],\n",
            "        [-10.6296,  -4.0150],\n",
            "        [ -9.8343,  -6.0976],\n",
            "        [ -5.0938, -11.6420],\n",
            "        [ -2.8262, -16.0294],\n",
            "        [ -2.9536, -15.7050],\n",
            "        [-10.4771,  -4.0588],\n",
            "        [-12.2253,  -2.8551],\n",
            "        [ -5.2779, -12.0346],\n",
            "        [-11.3184,  -3.7698],\n",
            "        [ -3.8188, -13.8491],\n",
            "        [-10.6016,  -4.1264],\n",
            "        [ -2.1393, -17.4846],\n",
            "        [-10.4985,  -4.0361],\n",
            "        [ -4.3314, -12.7557],\n",
            "        [-11.7270,  -3.2224],\n",
            "        [ -4.9773, -12.1018],\n",
            "        [-12.3540,  -2.7854],\n",
            "        [ -3.9927, -13.4034],\n",
            "        [-10.2056,  -4.5580],\n",
            "        [-11.2781,  -3.6202],\n",
            "        [-12.7118,  -2.5271],\n",
            "        [-11.4901,  -3.4266],\n",
            "        [-14.1470,  -1.5612],\n",
            "        [ -2.8645, -15.8811],\n",
            "        [-11.2098,  -3.6506],\n",
            "        [-11.1305,  -5.4986],\n",
            "        [ -9.8654,  -5.4557],\n",
            "        [-10.0965,  -4.3079],\n",
            "        [ -2.1778, -17.5200],\n",
            "        [ -2.9913, -15.7521],\n",
            "        [ -3.6425, -14.1417]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[-11.2791,  -5.1033],\n",
            "        [ -4.2909, -14.7409],\n",
            "        [-11.1906,  -4.6625],\n",
            "        [-11.6941,  -4.2754],\n",
            "        [-10.8709,  -5.6747],\n",
            "        [-13.4834,  -2.9647],\n",
            "        [ -9.1966,  -5.9156],\n",
            "        [-13.3763,  -3.9608],\n",
            "        [-13.2300,  -3.0746],\n",
            "        [-12.4980,  -3.9115],\n",
            "        [-10.9437,  -4.7671],\n",
            "        [-13.5352,  -2.9306],\n",
            "        [-11.8068,  -4.1156],\n",
            "        [ -3.9466, -15.6360],\n",
            "        [-14.5551,  -2.2240],\n",
            "        [ -3.6353, -16.1858],\n",
            "        [ -3.4327, -16.5546],\n",
            "        [ -3.7962, -15.8795],\n",
            "        [ -5.5555, -11.8319],\n",
            "        [ -5.6208, -11.8164],\n",
            "        [-10.7505,  -6.3513],\n",
            "        [ -2.4957, -18.8264],\n",
            "        [ -2.3314, -19.2870],\n",
            "        [-13.8587,  -2.7054],\n",
            "        [-12.2854,  -4.3002],\n",
            "        [-10.7674,  -5.2460],\n",
            "        [ -3.4844, -16.4087],\n",
            "        [ -2.7517, -18.1123],\n",
            "        [-11.5017,  -4.7327],\n",
            "        [-10.2406,  -5.8861],\n",
            "        [ -2.3298, -19.0696],\n",
            "        [ -4.2459, -14.7023],\n",
            "        [-10.7797,  -5.3235],\n",
            "        [-11.8519,  -4.0148],\n",
            "        [-11.3274,  -4.3453],\n",
            "        [ -5.1553, -12.6780],\n",
            "        [ -4.7298, -13.6782],\n",
            "        [-13.6959,  -3.2854],\n",
            "        [-13.2518,  -3.1537],\n",
            "        [-10.7429,  -4.7688],\n",
            "        [ -4.3590, -14.5564],\n",
            "        [-10.7172,  -6.1972],\n",
            "        [-13.2003,  -3.1565],\n",
            "        [ -5.0318, -13.4445],\n",
            "        [-13.6129,  -2.8710],\n",
            "        [ -4.1228, -15.1357],\n",
            "        [-12.2769,  -4.0135],\n",
            "        [-10.9997,  -4.5817],\n",
            "        [-10.4819,  -4.9870],\n",
            "        [ -1.0401, -22.0649],\n",
            "        [ -9.3746,  -6.8873],\n",
            "        [ -4.3325, -14.7699],\n",
            "        [-11.0687,  -5.1460],\n",
            "        [-11.7353,  -4.2709],\n",
            "        [ -3.6682, -16.1027],\n",
            "        [-11.9710,  -4.4940],\n",
            "        [-12.0516,  -3.9564],\n",
            "        [-12.2538,  -3.7533],\n",
            "        [ -5.8927, -12.2004],\n",
            "        [-15.4618,  -1.6193],\n",
            "        [ -3.6499, -16.3416],\n",
            "        [ -4.3942, -14.4805],\n",
            "        [ -4.7373, -13.8945],\n",
            "        [-10.9274,  -4.7404]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[-11.0549,  -3.4441],\n",
            "        [ -8.4114,  -7.5946],\n",
            "        [-12.4352,  -3.7118],\n",
            "        [ -2.4199, -17.0705],\n",
            "        [ -3.6486, -14.2875],\n",
            "        [ -5.2870, -10.5189],\n",
            "        [ -9.9784,  -4.2145],\n",
            "        [ -2.1843, -17.3910],\n",
            "        [ -4.1817, -13.0725],\n",
            "        [ -3.4377, -14.6018],\n",
            "        [ -9.1856,  -5.2745],\n",
            "        [ -5.0859, -11.0806],\n",
            "        [ -2.7898, -16.0786],\n",
            "        [ -3.0433, -15.5362],\n",
            "        [-10.9770,  -3.4935],\n",
            "        [ -4.7952, -11.5866],\n",
            "        [ -9.0181,  -6.2417],\n",
            "        [-10.1613,  -4.4738],\n",
            "        [-11.2470,  -3.3793],\n",
            "        [ -4.4055, -12.6275],\n",
            "        [-11.6359,  -3.3541],\n",
            "        [ -1.9936, -17.8396],\n",
            "        [ -6.3258,  -9.5689],\n",
            "        [ -8.4912,  -5.4648],\n",
            "        [-10.6833,  -3.6974],\n",
            "        [-10.8540,  -3.5358],\n",
            "        [ -4.4134, -12.5043],\n",
            "        [ -4.6484, -11.8608],\n",
            "        [-12.8019,  -2.1854],\n",
            "        [ -4.6705, -11.8967],\n",
            "        [ -4.5424, -12.2645],\n",
            "        [ -4.5253, -12.2400],\n",
            "        [ -1.8164, -18.3661],\n",
            "        [-11.4052,  -3.5992],\n",
            "        [-10.4037,  -3.9071],\n",
            "        [-10.6260,  -3.8052],\n",
            "        [-10.2685,  -3.9892],\n",
            "        [-12.3034,  -2.6061],\n",
            "        [ -9.0717,  -6.2300],\n",
            "        [-10.9638,  -4.6197],\n",
            "        [-12.1105,  -2.6782],\n",
            "        [-11.1098,  -3.3148],\n",
            "        [ -2.7126, -16.2792],\n",
            "        [-10.0963,  -3.9728],\n",
            "        [ -9.8149,  -5.5623],\n",
            "        [-12.4952,  -2.4383],\n",
            "        [-11.5834,  -3.1616],\n",
            "        [ -2.8833, -15.8291],\n",
            "        [ -7.1086,  -8.5825],\n",
            "        [-10.0198,  -4.1293],\n",
            "        [ -9.4038,  -4.9583],\n",
            "        [ -4.2979, -12.8385],\n",
            "        [ -6.4167,  -8.3337],\n",
            "        [-12.1440,  -2.6447],\n",
            "        [-10.7013,  -3.6826],\n",
            "        [ -9.7549,  -4.5897],\n",
            "        [ -4.1275, -13.3043],\n",
            "        [-11.1833,  -3.4095],\n",
            "        [ -3.0410, -15.7556],\n",
            "        [ -9.4043,  -4.5220],\n",
            "        [ -4.2747, -12.7889],\n",
            "        [ -4.3166, -13.4695],\n",
            "        [-12.0017,  -3.6165],\n",
            "        [-10.0494,  -4.8517]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[-10.9695,  -4.3314],\n",
            "        [ -1.3651, -21.0410],\n",
            "        [ -9.9452,  -5.5605],\n",
            "        [-10.2121,  -4.8461],\n",
            "        [ -5.4322, -12.3415],\n",
            "        [ -3.7184, -15.5969],\n",
            "        [-11.7324,  -3.6809],\n",
            "        [ -5.4934, -11.8393],\n",
            "        [-10.7763,  -4.4321],\n",
            "        [ -4.0068, -15.0856],\n",
            "        [-12.5769,  -3.5715],\n",
            "        [ -8.9739,  -6.0595],\n",
            "        [ -3.3352, -16.7171],\n",
            "        [ -2.5891, -18.2162],\n",
            "        [ -3.7346, -15.6635],\n",
            "        [ -3.2750, -16.9215],\n",
            "        [-12.5752,  -3.3295],\n",
            "        [ -4.8102, -13.2758],\n",
            "        [ -3.7412, -15.5941],\n",
            "        [-11.2655,  -4.7077],\n",
            "        [ -4.1094, -14.8413],\n",
            "        [ -3.1671, -16.7804],\n",
            "        [ -3.1777, -16.9688],\n",
            "        [ -8.7245,  -7.1443],\n",
            "        [ -3.5012, -16.3079],\n",
            "        [ -5.3806, -12.0396],\n",
            "        [-13.5130,  -2.9145],\n",
            "        [ -5.0230, -13.3195],\n",
            "        [ -3.2057, -17.0015],\n",
            "        [-12.9700,  -2.8277],\n",
            "        [ -7.6236, -11.0103],\n",
            "        [-11.3086,  -4.4216],\n",
            "        [ -2.5319, -18.3348],\n",
            "        [-13.2863,  -2.6409],\n",
            "        [ -4.9655, -12.8871],\n",
            "        [ -5.5852, -11.5079],\n",
            "        [ -4.9810, -13.8289],\n",
            "        [ -3.3640, -16.4060],\n",
            "        [ -4.8783, -13.1883],\n",
            "        [-12.5532,  -3.1366],\n",
            "        [-11.1416,  -4.1427],\n",
            "        [-12.3399,  -3.3726],\n",
            "        [ -4.1876, -14.7609],\n",
            "        [ -2.6542, -18.0306],\n",
            "        [-10.4945,  -4.8570],\n",
            "        [-11.2215,  -4.2642],\n",
            "        [-11.4790,  -3.8692],\n",
            "        [-12.8172,  -3.1031],\n",
            "        [ -6.7959,  -9.1436],\n",
            "        [ -3.5277, -16.0961],\n",
            "        [-14.1700,  -2.0450],\n",
            "        [ -3.9036, -15.3941],\n",
            "        [-13.0236,  -2.8586],\n",
            "        [-11.8926,  -3.8369],\n",
            "        [ -4.3945, -14.1912],\n",
            "        [-13.6354,  -2.4665],\n",
            "        [-12.5947,  -3.1443],\n",
            "        [-12.6458,  -3.1765],\n",
            "        [ -4.4953, -14.0544],\n",
            "        [ -8.3938,  -7.5232],\n",
            "        [-12.3016,  -3.4176],\n",
            "        [ -5.6145, -11.4263],\n",
            "        [-13.9654,  -2.6235],\n",
            "        [ -5.6938, -11.5931]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[-12.5069,  -3.2532],\n",
            "        [ -4.3038, -13.7869],\n",
            "        [ -9.9331,  -5.2113],\n",
            "        [-10.1336,  -5.0430],\n",
            "        [-11.9852,  -3.6510],\n",
            "        [-11.4512,  -3.9880],\n",
            "        [ -2.8955, -16.9072],\n",
            "        [ -3.8880, -14.5679],\n",
            "        [ -4.4985, -14.2950],\n",
            "        [-12.8440,  -3.1623],\n",
            "        [-11.5152,  -4.3659],\n",
            "        [ -1.9779, -19.1156],\n",
            "        [-10.6059,  -4.6746],\n",
            "        [ -5.4124, -12.0495],\n",
            "        [ -4.5239, -13.1391],\n",
            "        [-11.7068,  -3.8903],\n",
            "        [-10.0080,  -5.0019],\n",
            "        [ -5.0936, -11.8385],\n",
            "        [ -4.4576, -13.3389],\n",
            "        [ -3.3981, -15.7158],\n",
            "        [-13.9901,  -2.3040],\n",
            "        [ -3.6443, -15.1079],\n",
            "        [-11.3003,  -4.2185],\n",
            "        [ -4.6977, -12.8861],\n",
            "        [ -5.1103, -12.3667],\n",
            "        [-11.1086,  -4.4856],\n",
            "        [ -3.7913, -14.7915],\n",
            "        [-11.3674,  -4.1216],\n",
            "        [-10.7599,  -4.5409],\n",
            "        [-12.8586,  -3.1044],\n",
            "        [ -9.4502,  -6.2122],\n",
            "        [ -4.2481, -13.7662],\n",
            "        [-10.8280,  -5.3406],\n",
            "        [ -1.9952, -18.6938],\n",
            "        [-10.5206,  -5.6331],\n",
            "        [ -4.2107, -13.7771],\n",
            "        [-16.0541,  -0.8677],\n",
            "        [ -4.2255, -13.7800],\n",
            "        [ -5.3809, -11.1362],\n",
            "        [ -4.2325, -13.8323],\n",
            "        [ -9.4144,  -6.1527],\n",
            "        [-10.2718,  -5.3493],\n",
            "        [ -1.9582, -18.8600],\n",
            "        [-10.6935,  -4.5325],\n",
            "        [-11.4722,  -3.9513],\n",
            "        [-12.9264,  -3.0190],\n",
            "        [ -2.6668, -17.2277],\n",
            "        [ -8.7034,  -5.9948],\n",
            "        [ -2.8801, -16.8045],\n",
            "        [-12.3299,  -3.8124],\n",
            "        [-10.3524,  -4.7135],\n",
            "        [-10.9409,  -4.3642],\n",
            "        [ -3.4497, -15.7071],\n",
            "        [-11.7339,  -3.9503],\n",
            "        [-10.4167,  -5.6139],\n",
            "        [ -3.6365, -15.0967],\n",
            "        [-11.3420,  -4.0960],\n",
            "        [ -5.5011, -11.6763],\n",
            "        [ -3.2832, -16.2813],\n",
            "        [ -4.2526, -13.6834],\n",
            "        [ -3.4388, -15.5954],\n",
            "        [ -3.9939, -14.3763],\n",
            "        [-10.4761,  -4.7311],\n",
            "        [ -9.9273,  -5.2739]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[-10.5640,  -4.3187],\n",
            "        [ -9.8682,  -4.8442],\n",
            "        [ -4.9683, -11.5036],\n",
            "        [ -5.9962,  -9.5180],\n",
            "        [ -2.6838, -16.6943],\n",
            "        [ -3.4513, -15.1680],\n",
            "        [-11.2392,  -4.1164],\n",
            "        [ -4.6197, -12.1871],\n",
            "        [ -9.0740,  -5.9681],\n",
            "        [-13.6461,  -2.1101],\n",
            "        [ -5.1410, -11.2168],\n",
            "        [ -1.6823, -18.7885],\n",
            "        [-10.1121,  -4.4271],\n",
            "        [ -4.2212, -13.2889],\n",
            "        [ -9.6878,  -5.2317],\n",
            "        [ -4.2818, -13.0288],\n",
            "        [ -2.9346, -16.0603],\n",
            "        [ -4.4988, -12.5410],\n",
            "        [-10.4459,  -4.1987],\n",
            "        [ -4.8868, -11.6975],\n",
            "        [-10.2060,  -4.3418],\n",
            "        [ -2.6697, -16.6190],\n",
            "        [-11.2777,  -3.4775],\n",
            "        [ -4.3354, -12.9066],\n",
            "        [ -3.0142, -15.9651],\n",
            "        [ -4.2976, -13.1185],\n",
            "        [-12.2669,  -2.8549],\n",
            "        [ -5.3907, -10.5441],\n",
            "        [ -3.8476, -14.0782],\n",
            "        [-13.2919,  -2.3768],\n",
            "        [-11.3855,  -3.7101],\n",
            "        [-11.7942,  -3.1526],\n",
            "        [ -3.0648, -15.7028],\n",
            "        [-10.6644,  -3.9708],\n",
            "        [ -5.0696, -11.2854],\n",
            "        [ -3.9710, -13.8578],\n",
            "        [ -4.2185, -13.1584],\n",
            "        [ -3.2366, -15.5122],\n",
            "        [-11.0461,  -3.6411],\n",
            "        [ -4.3437, -12.9366],\n",
            "        [ -5.6646, -10.3323],\n",
            "        [ -5.2021, -11.0771],\n",
            "        [ -9.9572,  -4.4523],\n",
            "        [ -4.7760, -11.9300],\n",
            "        [ -5.1290, -11.6986],\n",
            "        [-11.1453,  -3.5678],\n",
            "        [-12.2519,  -2.8505],\n",
            "        [-11.4396,  -3.3982],\n",
            "        [-11.7405,  -3.2905],\n",
            "        [-10.2329,  -4.3864],\n",
            "        [-11.0322,  -3.7877],\n",
            "        [-11.3288,  -4.0570],\n",
            "        [-13.5560,  -1.9518],\n",
            "        [-10.1506,  -4.2790],\n",
            "        [ -4.4971, -12.6366],\n",
            "        [-10.3457,  -4.5137],\n",
            "        [ -2.6357, -16.7535],\n",
            "        [-10.6716,  -3.8960],\n",
            "        [ -3.2524, -15.3311],\n",
            "        [ -2.0160, -18.6853],\n",
            "        [-11.0419,  -4.4172],\n",
            "        [-12.2824,  -2.8065],\n",
            "        [-10.8742,  -4.4338],\n",
            "        [ -9.3994,  -4.9158]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[ -3.9010, -13.6045],\n",
            "        [-11.3285,  -3.5001],\n",
            "        [-11.2909,  -3.7398],\n",
            "        [-10.7820,  -3.5152],\n",
            "        [ -4.5525, -12.0805],\n",
            "        [-12.7316,  -2.1729],\n",
            "        [-10.9572,  -3.4163],\n",
            "        [ -3.9181, -13.5185],\n",
            "        [ -3.7717, -13.8724],\n",
            "        [ -4.3416, -12.5889],\n",
            "        [ -4.5956, -12.4087],\n",
            "        [ -5.1729, -10.7446],\n",
            "        [ -2.9988, -15.6578],\n",
            "        [ -3.3457, -14.7890],\n",
            "        [ -9.6095,  -4.2993],\n",
            "        [ -3.9179, -13.6432],\n",
            "        [ -2.2208, -17.3613],\n",
            "        [ -9.9345,  -4.5311],\n",
            "        [ -5.1527, -11.4523],\n",
            "        [ -4.7316, -12.3404],\n",
            "        [-13.5935,  -1.7310],\n",
            "        [-12.0071,  -2.8035],\n",
            "        [ -9.8596,  -4.0943],\n",
            "        [-10.3308,  -4.7280],\n",
            "        [-10.5209,  -3.7512],\n",
            "        [-12.2426,  -2.7697],\n",
            "        [ -3.7110, -14.0440],\n",
            "        [ -8.9004,  -5.4315],\n",
            "        [ -3.1253, -15.3806],\n",
            "        [ -3.7023, -14.0454],\n",
            "        [ -2.7989, -15.9983],\n",
            "        [-11.2906,  -3.2015],\n",
            "        [ -3.9275, -13.5810],\n",
            "        [ -3.5009, -14.5034],\n",
            "        [-11.7007,  -2.9071],\n",
            "        [ -4.9736, -11.1035],\n",
            "        [ -1.9746, -18.1515],\n",
            "        [-11.3452,  -4.0766],\n",
            "        [ -2.9612, -15.8373],\n",
            "        [-11.6185,  -2.9171],\n",
            "        [ -4.0388, -13.4761],\n",
            "        [ -3.4767, -14.4294],\n",
            "        [ -3.9335, -13.6583],\n",
            "        [ -3.5083, -14.5352],\n",
            "        [ -4.5064, -12.4569],\n",
            "        [-11.5357,  -2.9842],\n",
            "        [ -5.5176,  -9.9563],\n",
            "        [ -9.8735,  -4.5775],\n",
            "        [ -4.7418, -13.0935],\n",
            "        [ -4.3820, -12.4939],\n",
            "        [ -3.8186, -13.7801],\n",
            "        [-10.4503,  -3.7148],\n",
            "        [ -4.1005, -13.5897],\n",
            "        [ -4.8705, -11.3445],\n",
            "        [ -9.9110,  -5.6713],\n",
            "        [ -4.1738, -12.8130],\n",
            "        [-10.2258,  -4.1245],\n",
            "        [ -9.7416,  -5.3591],\n",
            "        [ -9.1941,  -5.4856],\n",
            "        [-11.1820,  -3.2189],\n",
            "        [-11.3608,  -3.1276],\n",
            "        [-10.8935,  -3.5951],\n",
            "        [ -4.0997, -13.1673],\n",
            "        [ -2.9033, -15.8036]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[-13.3400,  -2.3654],\n",
            "        [ -9.9529,  -6.0340],\n",
            "        [-11.5006,  -3.6863],\n",
            "        [ -7.8892,  -7.6945],\n",
            "        [ -3.3401, -15.3398],\n",
            "        [-11.6395,  -3.4859],\n",
            "        [-10.8359,  -4.0585],\n",
            "        [ -4.8106, -12.1510],\n",
            "        [ -3.0870, -15.9930],\n",
            "        [-10.3541,  -6.0097],\n",
            "        [ -4.4978, -12.8518],\n",
            "        [-10.1308,  -5.0193],\n",
            "        [-12.0118,  -3.3302],\n",
            "        [ -3.1982, -15.7655],\n",
            "        [-10.3329,  -4.5356],\n",
            "        [-14.8585,  -1.3658],\n",
            "        [-13.5422,  -2.2397],\n",
            "        [-12.2381,  -4.3563],\n",
            "        [-11.1596,  -3.9678],\n",
            "        [ -4.0671, -13.7896],\n",
            "        [-10.3387,  -4.4307],\n",
            "        [-11.3765,  -4.2259],\n",
            "        [ -3.6642, -14.5764],\n",
            "        [ -4.0788, -13.8450],\n",
            "        [-10.2638,  -4.7930],\n",
            "        [ -2.2194, -17.8450],\n",
            "        [ -3.0893, -15.9967],\n",
            "        [-10.8418,  -4.0326],\n",
            "        [ -2.8255, -16.4597],\n",
            "        [-11.5210,  -3.8054],\n",
            "        [-12.6593,  -2.9302],\n",
            "        [-11.5859,  -3.5265],\n",
            "        [ -3.0376, -16.1411],\n",
            "        [ -2.8375, -16.5873],\n",
            "        [ -3.6711, -14.6159],\n",
            "        [ -5.1136, -12.6573],\n",
            "        [ -4.9589, -11.7297],\n",
            "        [ -8.5130,  -7.5407],\n",
            "        [-11.9676,  -3.6335],\n",
            "        [-12.0449,  -3.5521],\n",
            "        [ -2.5197, -17.1772],\n",
            "        [ -4.1323, -13.6278],\n",
            "        [ -2.8154, -16.4595],\n",
            "        [-10.9196,  -4.0748],\n",
            "        [-12.6034,  -3.0153],\n",
            "        [-10.8583,  -4.0498],\n",
            "        [ -9.8663,  -5.4127],\n",
            "        [ -2.8794, -16.6924],\n",
            "        [ -3.8765, -14.7392],\n",
            "        [-10.0706,  -4.7580],\n",
            "        [-10.2350,  -4.8506],\n",
            "        [ -3.6558, -14.7157],\n",
            "        [-11.0075,  -3.9218],\n",
            "        [ -3.9311, -14.2565],\n",
            "        [ -9.2966,  -6.0216],\n",
            "        [-10.9594,  -4.8578],\n",
            "        [ -5.6097, -10.2647],\n",
            "        [ -3.8359, -14.1953],\n",
            "        [ -4.4605, -14.0225],\n",
            "        [ -2.2515, -17.7620],\n",
            "        [ -4.1488, -13.5188],\n",
            "        [-10.8007,  -4.0574],\n",
            "        [ -5.8867, -10.2288],\n",
            "        [-10.7910,  -4.1116]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[ -5.6580, -11.0645],\n",
            "        [ -3.2572, -15.7835],\n",
            "        [-13.8029,  -2.0973],\n",
            "        [ -3.9060, -14.2212],\n",
            "        [-12.9229,  -2.4476],\n",
            "        [ -3.7165, -14.6659],\n",
            "        [ -6.1131,  -9.9518],\n",
            "        [-10.5690,  -4.5260],\n",
            "        [-11.1358,  -3.7258],\n",
            "        [ -2.7270, -16.9731],\n",
            "        [ -3.0375, -16.1094],\n",
            "        [ -2.9759, -16.3002],\n",
            "        [ -3.4912, -15.2507],\n",
            "        [ -9.8110,  -5.2257],\n",
            "        [-11.9022,  -3.1635],\n",
            "        [-11.0904,  -4.0419],\n",
            "        [-11.9852,  -3.1004],\n",
            "        [-11.9209,  -3.1898],\n",
            "        [ -3.3879, -15.3691],\n",
            "        [-11.4133,  -4.0282],\n",
            "        [ -9.9377,  -5.4480],\n",
            "        [ -5.7410, -10.6089],\n",
            "        [-10.5611,  -4.0744],\n",
            "        [-12.5015,  -2.7850],\n",
            "        [ -3.0070, -16.2816],\n",
            "        [ -1.7724, -18.9788],\n",
            "        [-10.7871,  -5.1630],\n",
            "        [-10.3394,  -4.4075],\n",
            "        [ -2.6543, -17.1679],\n",
            "        [-13.2245,  -2.3694],\n",
            "        [-10.8221,  -4.1221],\n",
            "        [ -5.3821, -11.1498],\n",
            "        [ -9.0135,  -6.6648],\n",
            "        [ -4.0997, -14.4249],\n",
            "        [ -5.5237, -11.0701],\n",
            "        [ -3.3742, -15.4412],\n",
            "        [ -5.3026, -11.0432],\n",
            "        [-10.4983,  -4.8895],\n",
            "        [-10.7966,  -4.0030],\n",
            "        [ -3.5309, -15.4742],\n",
            "        [ -3.7122, -15.0544],\n",
            "        [-11.3616,  -3.4566],\n",
            "        [ -4.5109, -13.0124],\n",
            "        [-11.6060,  -3.3421],\n",
            "        [-11.0273,  -3.7712],\n",
            "        [ -4.2591, -13.4866],\n",
            "        [-12.4333,  -2.7756],\n",
            "        [ -3.4629, -15.1438],\n",
            "        [-10.2425,  -4.7700],\n",
            "        [ -3.1295, -15.9826],\n",
            "        [-12.2936,  -2.8672],\n",
            "        [ -3.8346, -14.3596],\n",
            "        [ -3.4870, -15.3329],\n",
            "        [-10.7263,  -3.9116],\n",
            "        [ -3.0723, -16.1427],\n",
            "        [ -9.6551,  -4.6894],\n",
            "        [-10.6720,  -4.0525],\n",
            "        [ -8.9085,  -5.5994],\n",
            "        [-11.4884,  -3.4263],\n",
            "        [ -3.0780, -16.0590],\n",
            "        [ -5.0294, -12.4675],\n",
            "        [ -3.9332, -14.1597],\n",
            "        [-11.2353,  -3.5530],\n",
            "        [-10.9745,  -3.9295]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[-11.0226,  -4.5504],\n",
            "        [-11.5818,  -3.7323],\n",
            "        [ -3.9206, -14.4911],\n",
            "        [ -4.2025, -13.9289],\n",
            "        [ -4.0260, -14.5211],\n",
            "        [-10.9756,  -3.9561],\n",
            "        [-13.1061,  -2.5426],\n",
            "        [ -4.4148, -13.4433],\n",
            "        [ -4.2146, -13.9183],\n",
            "        [ -3.6908, -15.0887],\n",
            "        [-12.9432,  -2.6756],\n",
            "        [-10.7221,  -4.1658],\n",
            "        [-10.9182,  -5.3816],\n",
            "        [-13.4965,  -2.2884],\n",
            "        [-10.8637,  -4.4189],\n",
            "        [-11.7141,  -3.4733],\n",
            "        [ -6.8740,  -9.0715],\n",
            "        [-12.6663,  -2.8775],\n",
            "        [ -4.7861, -12.5574],\n",
            "        [ -6.3629,  -9.0522],\n",
            "        [ -6.5352,  -9.9466],\n",
            "        [-10.6012,  -5.2842],\n",
            "        [ -5.0180, -12.1312],\n",
            "        [ -4.5638, -13.0695],\n",
            "        [-11.2790,  -3.7842],\n",
            "        [-11.3757,  -3.8013],\n",
            "        [-11.7744,  -3.7557],\n",
            "        [ -4.4389, -13.4410],\n",
            "        [ -3.0667, -16.3889],\n",
            "        [ -4.5841, -13.1010],\n",
            "        [-12.0518,  -3.3346],\n",
            "        [-12.8317,  -2.7756],\n",
            "        [ -3.4696, -15.5925],\n",
            "        [ -4.8020, -12.6358],\n",
            "        [ -3.7693, -14.9259],\n",
            "        [ -5.6242, -11.4120],\n",
            "        [ -3.6164, -15.2691],\n",
            "        [-11.5918,  -4.0007],\n",
            "        [ -4.8435, -12.4643],\n",
            "        [ -3.4288, -15.7025],\n",
            "        [-10.6603,  -4.1735],\n",
            "        [ -2.0788, -18.6071],\n",
            "        [ -2.5820, -17.5480],\n",
            "        [ -4.1326, -14.1538],\n",
            "        [ -4.8447, -12.5098],\n",
            "        [ -4.7068, -12.8037],\n",
            "        [ -6.1111, -10.2627],\n",
            "        [ -2.1134, -18.5885],\n",
            "        [ -4.1145, -14.2067],\n",
            "        [ -9.4422,  -6.9454],\n",
            "        [-10.6050,  -4.2443],\n",
            "        [ -5.0427, -12.1181],\n",
            "        [-11.8843,  -3.4089],\n",
            "        [ -3.5213, -15.3961],\n",
            "        [-11.4474,  -3.9141],\n",
            "        [ -1.7233, -19.6359],\n",
            "        [-14.5971,  -1.5863],\n",
            "        [ -3.8400, -14.5945],\n",
            "        [ -3.3373, -15.8906],\n",
            "        [ -4.2735, -13.8771],\n",
            "        [-11.3813,  -3.7919],\n",
            "        [-12.5343,  -2.9491],\n",
            "        [-11.6163,  -3.7113],\n",
            "        [ -5.2379, -11.5828]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[ -9.9537,  -5.5356],\n",
            "        [-10.4378,  -4.8709],\n",
            "        [ -3.9505, -13.7078],\n",
            "        [ -7.6454,  -6.9403],\n",
            "        [ -4.6153, -12.1185],\n",
            "        [-11.7423,  -3.1475],\n",
            "        [-11.2144,  -3.4707],\n",
            "        [ -4.0402, -13.3461],\n",
            "        [ -4.0753, -13.3808],\n",
            "        [ -4.2727, -12.9910],\n",
            "        [-12.4824,  -2.6170],\n",
            "        [-13.7991,  -1.8125],\n",
            "        [ -9.4220,  -5.0271],\n",
            "        [-10.1670,  -4.6774],\n",
            "        [-12.6134,  -2.5411],\n",
            "        [ -3.4665, -14.9332],\n",
            "        [-11.1648,  -3.4500],\n",
            "        [ -5.2631, -10.6345],\n",
            "        [-11.7457,  -3.2973],\n",
            "        [-12.8448,  -2.3586],\n",
            "        [ -1.9998, -18.3775],\n",
            "        [ -2.6843, -16.4650],\n",
            "        [-10.1568,  -4.6035],\n",
            "        [ -4.4750, -12.3639],\n",
            "        [ -9.9572,  -4.4636],\n",
            "        [ -3.3386, -15.0024],\n",
            "        [ -6.4130,  -9.2843],\n",
            "        [ -2.9871, -15.8562],\n",
            "        [ -5.6914,  -9.9199],\n",
            "        [-11.7437,  -3.4540],\n",
            "        [-11.9188,  -2.9937],\n",
            "        [ -2.8937, -16.3060],\n",
            "        [-10.4142,  -4.6288],\n",
            "        [ -3.8327, -13.7937],\n",
            "        [ -5.0858, -11.1750],\n",
            "        [-10.9661,  -4.0888],\n",
            "        [ -4.0902, -13.3391],\n",
            "        [ -4.5326, -12.1679],\n",
            "        [ -3.4898, -14.5838],\n",
            "        [ -4.2854, -12.8883],\n",
            "        [ -4.5559, -12.5313],\n",
            "        [-10.5076,  -4.0063],\n",
            "        [ -3.2535, -15.4101],\n",
            "        [-11.3380,  -3.3441],\n",
            "        [ -4.2243, -12.9336],\n",
            "        [ -2.5782, -16.6275],\n",
            "        [ -4.1166, -13.1840],\n",
            "        [-10.7580,  -4.4897],\n",
            "        [-11.4804,  -3.3782],\n",
            "        [ -3.2782, -15.0125],\n",
            "        [-12.5569,  -2.9391],\n",
            "        [ -9.4884,  -4.7599],\n",
            "        [ -3.1637, -15.4865],\n",
            "        [ -9.9470,  -5.1854],\n",
            "        [-10.3568,  -4.1370],\n",
            "        [ -9.0229,  -5.2131],\n",
            "        [ -2.1260, -17.7351],\n",
            "        [-12.0759,  -3.0105],\n",
            "        [ -8.5472,  -6.4261],\n",
            "        [-10.3174,  -4.4165],\n",
            "        [ -3.8328, -13.9112],\n",
            "        [-11.9074,  -3.2469],\n",
            "        [ -3.2585, -15.1411],\n",
            "        [ -4.8071, -11.5906]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[ -4.5260, -12.8462],\n",
            "        [-11.0860,  -4.6326],\n",
            "        [ -5.3047, -11.0429],\n",
            "        [-11.2177,  -3.6944],\n",
            "        [ -2.5678, -17.2323],\n",
            "        [ -4.3360, -13.1839],\n",
            "        [ -5.1737, -11.6523],\n",
            "        [-13.6686,  -1.9622],\n",
            "        [ -4.0638, -13.9231],\n",
            "        [-11.6344,  -4.3648],\n",
            "        [-11.2382,  -3.9179],\n",
            "        [ -9.9627,  -4.8868],\n",
            "        [-11.7784,  -3.5019],\n",
            "        [ -4.8044, -12.2629],\n",
            "        [ -3.3588, -15.5978],\n",
            "        [-10.3538,  -4.6826],\n",
            "        [ -4.3344, -13.3075],\n",
            "        [-10.3788,  -4.3697],\n",
            "        [ -3.1930, -15.9224],\n",
            "        [-11.4246,  -3.6835],\n",
            "        [ -3.3164, -15.6227],\n",
            "        [ -5.0736, -11.6078],\n",
            "        [-12.0862,  -3.0859],\n",
            "        [ -3.6534, -14.8768],\n",
            "        [-11.8972,  -3.5241],\n",
            "        [ -9.7881,  -5.4894],\n",
            "        [-12.0423,  -3.0585],\n",
            "        [-11.3477,  -4.1450],\n",
            "        [-11.9510,  -3.0756],\n",
            "        [ -4.5506, -12.8423],\n",
            "        [ -4.5791, -13.1434],\n",
            "        [-11.5008,  -4.1570],\n",
            "        [-10.8883,  -4.4184],\n",
            "        [-11.8006,  -3.6045],\n",
            "        [-11.2829,  -3.6103],\n",
            "        [-13.2938,  -2.1968],\n",
            "        [ -3.5955, -14.9658],\n",
            "        [-11.1925,  -4.2871],\n",
            "        [ -2.9492, -16.3306],\n",
            "        [ -3.7108, -14.7058],\n",
            "        [ -4.3916, -13.2220],\n",
            "        [ -9.7374,  -5.4078],\n",
            "        [-12.8936,  -2.5179],\n",
            "        [-10.8564,  -4.4451],\n",
            "        [ -3.0636, -16.2045],\n",
            "        [ -4.3991, -13.2364],\n",
            "        [-10.5038,  -5.1731],\n",
            "        [ -1.0436, -20.7726],\n",
            "        [ -3.6875, -14.6884],\n",
            "        [-10.4076,  -4.7595],\n",
            "        [ -5.4316, -10.8174],\n",
            "        [ -3.5680, -15.0312],\n",
            "        [-12.8846,  -2.4641],\n",
            "        [-11.6300,  -3.3366],\n",
            "        [ -2.5213, -17.5605],\n",
            "        [ -4.8112, -12.2693],\n",
            "        [ -4.8437, -12.1391],\n",
            "        [ -9.9712,  -5.5435],\n",
            "        [-11.5576,  -3.3281],\n",
            "        [ -8.2011,  -7.1014],\n",
            "        [-10.9660,  -4.4896],\n",
            "        [-11.4286,  -3.5632],\n",
            "        [ -7.5176,  -8.0506],\n",
            "        [ -6.2610,  -9.1479]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[-11.4194,  -3.9077],\n",
            "        [ -4.8752, -12.2753],\n",
            "        [-11.5125,  -3.8482],\n",
            "        [-11.6550,  -3.8639],\n",
            "        [-11.8005,  -3.6453],\n",
            "        [-13.0313,  -2.8168],\n",
            "        [-13.0659,  -2.9292],\n",
            "        [-11.3662,  -3.9193],\n",
            "        [ -3.4336, -15.4170],\n",
            "        [ -3.9461, -14.3370],\n",
            "        [ -2.5763, -17.3813],\n",
            "        [ -9.8499,  -6.8018],\n",
            "        [ -4.0418, -14.0793],\n",
            "        [ -3.6653, -14.8989],\n",
            "        [-11.3345,  -4.0511],\n",
            "        [-11.7310,  -4.1655],\n",
            "        [-13.4516,  -2.5220],\n",
            "        [ -4.3550, -13.7323],\n",
            "        [ -3.7491, -14.7119],\n",
            "        [ -2.7594, -17.0407],\n",
            "        [-11.2642,  -3.9472],\n",
            "        [-12.0430,  -3.4641],\n",
            "        [-11.1586,  -4.1121],\n",
            "        [-10.4305,  -4.5925],\n",
            "        [-11.2903,  -4.4756],\n",
            "        [ -3.6956, -15.0002],\n",
            "        [ -5.0747, -11.9489],\n",
            "        [ -4.8772, -12.1568],\n",
            "        [ -3.0528, -16.3758],\n",
            "        [ -1.9365, -18.7384],\n",
            "        [-14.1781,  -2.0413],\n",
            "        [-14.1910,  -2.0477],\n",
            "        [ -3.5847, -15.2966],\n",
            "        [ -3.8815, -14.4911],\n",
            "        [ -3.5635, -15.1510],\n",
            "        [-13.0447,  -2.8501],\n",
            "        [-10.1564,  -6.7766],\n",
            "        [-12.7299,  -3.0565],\n",
            "        [ -2.6541, -17.2763],\n",
            "        [ -4.0171, -14.1817],\n",
            "        [-11.3942,  -4.0353],\n",
            "        [ -3.1316, -16.3297],\n",
            "        [ -9.1134,  -5.6469],\n",
            "        [ -4.8528, -12.2665],\n",
            "        [-14.2218,  -2.0181],\n",
            "        [ -4.6393, -13.1606],\n",
            "        [ -3.8822, -14.5132],\n",
            "        [ -3.6060, -15.1519],\n",
            "        [ -3.9288, -14.3439],\n",
            "        [ -3.7399, -14.8174],\n",
            "        [ -4.8849, -12.1968],\n",
            "        [ -4.7438, -12.4928],\n",
            "        [ -4.2840, -13.9063],\n",
            "        [ -9.2536,  -5.8411],\n",
            "        [ -2.8124, -17.0687],\n",
            "        [ -9.9774,  -5.0328],\n",
            "        [ -9.8072,  -5.2714],\n",
            "        [-11.2039,  -4.3338],\n",
            "        [ -2.8354, -16.8192],\n",
            "        [ -4.1299, -13.8714],\n",
            "        [ -4.7392, -12.7040],\n",
            "        [-11.2019,  -4.3860],\n",
            "        [-12.2297,  -4.1575],\n",
            "        [ -9.3496,  -7.2411]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[-12.8937,  -2.8724],\n",
            "        [-13.6915,  -2.3351],\n",
            "        [-12.9634,  -2.8691],\n",
            "        [ -3.6482, -15.2117],\n",
            "        [ -2.5630, -17.8044],\n",
            "        [ -3.7638, -15.0593],\n",
            "        [ -2.1238, -18.8446],\n",
            "        [-11.1784,  -4.2632],\n",
            "        [-10.1772,  -4.7353],\n",
            "        [-10.2844,  -5.4387],\n",
            "        [-11.7981,  -4.2462],\n",
            "        [-10.9698,  -4.4109],\n",
            "        [-11.4409,  -3.8636],\n",
            "        [-12.2183,  -3.3594],\n",
            "        [-11.5647,  -3.7540],\n",
            "        [ -4.6692, -12.9431],\n",
            "        [ -4.4233, -13.5172],\n",
            "        [ -3.3531, -15.9170],\n",
            "        [-11.5820,  -3.9820],\n",
            "        [ -3.4416, -15.7399],\n",
            "        [ -3.9226, -14.7994],\n",
            "        [ -5.2560, -11.8245],\n",
            "        [ -3.9292, -14.6828],\n",
            "        [ -3.0287, -16.9419],\n",
            "        [ -4.0331, -14.4442],\n",
            "        [ -2.1425, -18.7599],\n",
            "        [-10.8424,  -4.3077],\n",
            "        [-12.3392,  -3.3242],\n",
            "        [-13.8894,  -2.2508],\n",
            "        [ -3.9142, -14.9853],\n",
            "        [-11.0309,  -4.1388],\n",
            "        [ -3.7731, -15.0436],\n",
            "        [ -7.5419,  -8.7090],\n",
            "        [ -2.7237, -17.3558],\n",
            "        [ -4.1948, -14.1740],\n",
            "        [-11.2916,  -4.0850],\n",
            "        [-11.0631,  -4.3061],\n",
            "        [-10.1696,  -5.5826],\n",
            "        [ -9.2563,  -7.1878],\n",
            "        [ -5.0793, -12.4155],\n",
            "        [ -3.7817, -15.0963],\n",
            "        [ -4.1099, -14.2239],\n",
            "        [ -4.1902, -14.4338],\n",
            "        [-10.4359,  -5.1169],\n",
            "        [-10.5553,  -5.0429],\n",
            "        [ -3.2395, -16.3808],\n",
            "        [-13.9914,  -2.1506],\n",
            "        [ -4.3828, -13.6474],\n",
            "        [-10.0777,  -5.3391],\n",
            "        [-12.8390,  -2.9644],\n",
            "        [-11.4321,  -3.9294],\n",
            "        [-12.9755,  -2.8201],\n",
            "        [ -4.1439, -14.2003],\n",
            "        [-12.2047,  -3.3788],\n",
            "        [ -2.2829, -18.4536],\n",
            "        [-12.9783,  -3.0116],\n",
            "        [ -3.0271, -16.5878],\n",
            "        [ -4.2467, -13.9127],\n",
            "        [-12.3086,  -3.3464],\n",
            "        [ -4.5927, -13.1116],\n",
            "        [ -5.2193, -11.8259],\n",
            "        [-12.0399,  -3.4759],\n",
            "        [ -5.3877, -11.4663],\n",
            "        [-11.3331,  -3.9801]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[-11.0247,  -3.7686],\n",
            "        [-10.1229,  -5.0230],\n",
            "        [ -3.0441, -16.5079],\n",
            "        [ -3.3561, -15.7682],\n",
            "        [-11.2071,  -3.8795],\n",
            "        [ -3.9281, -14.4368],\n",
            "        [-13.4366,  -2.2479],\n",
            "        [-10.7945,  -4.0534],\n",
            "        [ -3.7012, -14.8559],\n",
            "        [-12.6767,  -2.7856],\n",
            "        [ -4.1733, -14.1921],\n",
            "        [-11.5219,  -3.4605],\n",
            "        [ -2.7994, -16.9416],\n",
            "        [-11.2672,  -3.7130],\n",
            "        [ -2.7719, -16.9676],\n",
            "        [-10.0940,  -5.0359],\n",
            "        [-12.1045,  -3.1279],\n",
            "        [ -4.4211, -13.4036],\n",
            "        [ -7.8849,  -8.1910],\n",
            "        [-12.1131,  -3.1014],\n",
            "        [ -4.2051, -13.8366],\n",
            "        [-11.9652,  -3.5963],\n",
            "        [ -2.5906, -17.3788],\n",
            "        [-10.9439,  -3.8503],\n",
            "        [-10.4465,  -4.5328],\n",
            "        [ -2.7257, -17.1978],\n",
            "        [ -5.8380, -10.2015],\n",
            "        [ -2.7986, -17.0185],\n",
            "        [-11.7755,  -3.3110],\n",
            "        [ -6.6503,  -9.5600],\n",
            "        [-10.8382,  -4.6075],\n",
            "        [-11.9754,  -3.1931],\n",
            "        [-12.1891,  -3.1123],\n",
            "        [-12.0559,  -3.1653],\n",
            "        [ -2.1959, -18.2552],\n",
            "        [ -4.5405, -13.1574],\n",
            "        [ -4.7668, -12.5236],\n",
            "        [ -4.4730, -13.2138],\n",
            "        [-11.6160,  -3.7667],\n",
            "        [ -7.0881,  -9.6188],\n",
            "        [ -3.0209, -16.5058],\n",
            "        [ -8.7924,  -5.6835],\n",
            "        [-10.3230,  -4.7830],\n",
            "        [ -3.3386, -15.7104],\n",
            "        [ -2.8628, -16.7377],\n",
            "        [ -4.8878, -12.2978],\n",
            "        [-13.3700,  -2.1892],\n",
            "        [ -2.8930, -16.8686],\n",
            "        [-11.8471,  -3.3982],\n",
            "        [-11.8980,  -3.5043],\n",
            "        [ -7.3450,  -8.5226],\n",
            "        [ -3.9048, -14.5705],\n",
            "        [ -9.6039,  -5.0374],\n",
            "        [-12.0033,  -3.2432],\n",
            "        [-10.6730,  -4.3434],\n",
            "        [-11.2365,  -3.6499],\n",
            "        [ -3.1548, -16.0812],\n",
            "        [ -4.9776, -12.0389],\n",
            "        [ -3.8453, -14.6342],\n",
            "        [-11.5931,  -3.3934],\n",
            "        [ -4.1030, -14.1310],\n",
            "        [ -9.2190,  -6.3858],\n",
            "        [-12.3564,  -3.0089],\n",
            "        [ -3.0897, -16.3250]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[-11.3103,  -3.9324],\n",
            "        [-11.8142,  -3.3217],\n",
            "        [ -2.4042, -17.6458],\n",
            "        [ -3.2454, -15.8689],\n",
            "        [ -6.1952, -11.0956],\n",
            "        [ -3.2394, -16.0320],\n",
            "        [ -8.6521,  -7.3083],\n",
            "        [-12.3333,  -3.4047],\n",
            "        [ -4.0487, -14.0942],\n",
            "        [-10.3794,  -4.8778],\n",
            "        [-10.5320,  -4.3024],\n",
            "        [ -1.8065, -19.0004],\n",
            "        [ -2.6637, -17.1637],\n",
            "        [-10.4388,  -4.5429],\n",
            "        [-12.8919,  -2.6165],\n",
            "        [-12.9458,  -2.5867],\n",
            "        [ -4.1581, -13.8532],\n",
            "        [ -4.8582, -12.2960],\n",
            "        [ -9.2384,  -5.8947],\n",
            "        [-12.4321,  -2.9219],\n",
            "        [-11.3110,  -3.7819],\n",
            "        [ -2.3420, -17.8280],\n",
            "        [-12.3678,  -3.4081],\n",
            "        [ -3.8657, -14.3944],\n",
            "        [ -3.9976, -14.6917],\n",
            "        [-11.1939,  -3.8909],\n",
            "        [-12.4617,  -2.8986],\n",
            "        [ -8.6495,  -7.0031],\n",
            "        [-11.8913,  -3.3125],\n",
            "        [-11.4979,  -3.7790],\n",
            "        [ -4.9179, -12.0168],\n",
            "        [ -4.7510, -12.4716],\n",
            "        [-11.1779,  -3.9345],\n",
            "        [ -3.2785, -15.8546],\n",
            "        [ -2.6417, -17.2506],\n",
            "        [ -6.2280, -11.5157],\n",
            "        [-11.1329,  -4.0688],\n",
            "        [ -9.7580,  -5.3521],\n",
            "        [-11.7326,  -3.3932],\n",
            "        [-12.3316,  -3.0025],\n",
            "        [ -4.9236, -12.0758],\n",
            "        [-10.7737,  -4.4766],\n",
            "        [-13.2439,  -2.4103],\n",
            "        [-11.9251,  -3.7686],\n",
            "        [ -3.7769, -14.8092],\n",
            "        [ -3.0047, -16.4030],\n",
            "        [-13.4482,  -2.9900],\n",
            "        [ -3.8563, -14.6533],\n",
            "        [ -6.7834,  -9.7213],\n",
            "        [-11.9102,  -3.2851],\n",
            "        [ -4.8595, -12.2520],\n",
            "        [ -8.2140,  -8.0467],\n",
            "        [ -6.0418,  -9.5217],\n",
            "        [ -9.9893,  -5.1034],\n",
            "        [-12.9235,  -2.6235],\n",
            "        [ -3.4671, -15.3231],\n",
            "        [ -3.4113, -15.4108],\n",
            "        [-11.0457,  -3.8964],\n",
            "        [ -2.7375, -16.9286],\n",
            "        [ -3.7671, -14.8705],\n",
            "        [-11.5518,  -3.5331],\n",
            "        [ -4.1821, -13.8150],\n",
            "        [-11.3863,  -4.0840],\n",
            "        [ -4.4356, -13.2288]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[-10.3902,  -4.7558],\n",
            "        [ -5.2337, -12.4581],\n",
            "        [ -3.9618, -14.6849],\n",
            "        [-11.5387,  -3.4716],\n",
            "        [-10.5724,  -4.6468],\n",
            "        [ -5.1080, -12.1947],\n",
            "        [ -5.4567, -11.3169],\n",
            "        [-10.7817,  -4.2851],\n",
            "        [ -3.6407, -15.3988],\n",
            "        [ -5.2755, -11.8139],\n",
            "        [ -1.5795, -19.9172],\n",
            "        [-13.5995,  -2.1170],\n",
            "        [ -1.8497, -19.3467],\n",
            "        [ -4.0888, -14.4047],\n",
            "        [ -3.2387, -16.3655],\n",
            "        [ -2.8115, -17.3661],\n",
            "        [-10.7720,  -4.0779],\n",
            "        [-13.4838,  -2.3210],\n",
            "        [-11.7092,  -3.7759],\n",
            "        [ -4.1312, -14.2783],\n",
            "        [ -4.2115, -14.1902],\n",
            "        [ -4.3474, -13.8995],\n",
            "        [-12.5342,  -3.1967],\n",
            "        [-11.0632,  -4.2109],\n",
            "        [-11.3070,  -3.8544],\n",
            "        [-10.3426,  -5.1767],\n",
            "        [ -4.3762, -13.7796],\n",
            "        [-11.4549,  -3.6021],\n",
            "        [ -5.2678, -11.6936],\n",
            "        [ -3.6837, -15.2522],\n",
            "        [ -4.9561, -12.4823],\n",
            "        [ -4.6285, -13.2333],\n",
            "        [ -6.0856,  -9.8571],\n",
            "        [-10.3504,  -4.9492],\n",
            "        [ -4.7956, -12.7721],\n",
            "        [ -9.8843,  -5.4565],\n",
            "        [ -4.2507, -14.0169],\n",
            "        [-11.9009,  -3.2941],\n",
            "        [ -4.2352, -13.9321],\n",
            "        [ -7.1738,  -8.2203],\n",
            "        [ -4.2666, -14.0746],\n",
            "        [-13.5966,  -2.1271],\n",
            "        [ -6.1452,  -9.8726],\n",
            "        [ -2.8448, -17.2831],\n",
            "        [ -5.1795, -12.3124],\n",
            "        [-13.2522,  -2.3527],\n",
            "        [ -5.0276, -12.3269],\n",
            "        [ -4.7133, -12.9746],\n",
            "        [-12.1839,  -3.0890],\n",
            "        [-11.9950,  -3.7237],\n",
            "        [ -7.0449,  -9.3772],\n",
            "        [-11.8241,  -3.4205],\n",
            "        [-12.1305,  -3.1766],\n",
            "        [ -5.3926, -11.4456],\n",
            "        [-12.7875,  -2.7482],\n",
            "        [-12.2575,  -3.3427],\n",
            "        [ -5.7697, -10.5595],\n",
            "        [-12.4320,  -2.9413],\n",
            "        [ -8.4350,  -8.5002],\n",
            "        [ -5.3196, -11.7788],\n",
            "        [ -4.1459, -14.5153],\n",
            "        [ -1.6852, -19.7215],\n",
            "        [-11.2961,  -4.7485],\n",
            "        [ -8.1450,  -8.3941]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[ -3.9696, -14.1282],\n",
            "        [-12.2955,  -2.9156],\n",
            "        [ -8.9156,  -6.3642],\n",
            "        [ -4.3634, -13.2844],\n",
            "        [-10.9870,  -3.7884],\n",
            "        [-11.0705,  -3.7996],\n",
            "        [ -2.4357, -17.5725],\n",
            "        [ -3.9473, -14.1981],\n",
            "        [ -2.6658, -16.9654],\n",
            "        [-11.9125,  -3.4138],\n",
            "        [-12.6156,  -2.7984],\n",
            "        [-10.7248,  -4.1001],\n",
            "        [ -3.7534, -14.9057],\n",
            "        [ -4.3229, -13.2123],\n",
            "        [ -5.0057, -11.8240],\n",
            "        [ -3.5115, -15.2480],\n",
            "        [-10.4759,  -4.5937],\n",
            "        [ -9.4179,  -5.9996],\n",
            "        [-13.2782,  -2.2672],\n",
            "        [ -5.4306, -11.3939],\n",
            "        [-11.3932,  -3.9173],\n",
            "        [-11.7666,  -3.9965],\n",
            "        [ -2.7177, -16.8395],\n",
            "        [-12.0645,  -3.2263],\n",
            "        [ -2.6617, -17.1385],\n",
            "        [-11.6840,  -3.3892],\n",
            "        [-13.5182,  -2.1068],\n",
            "        [ -4.1517, -14.4986],\n",
            "        [ -3.0366, -16.3143],\n",
            "        [ -3.1632, -16.0183],\n",
            "        [-11.5577,  -3.4372],\n",
            "        [-11.5231,  -3.4683],\n",
            "        [ -9.9550,  -5.9461],\n",
            "        [ -3.2515, -15.8046],\n",
            "        [-11.9349,  -3.2755],\n",
            "        [ -9.3632,  -5.8635],\n",
            "        [ -2.4116, -17.5898],\n",
            "        [ -9.9664,  -5.0027],\n",
            "        [ -3.2253, -16.0490],\n",
            "        [ -3.0502, -16.1840],\n",
            "        [ -3.9178, -14.2254],\n",
            "        [ -4.3218, -13.3660],\n",
            "        [ -9.3533,  -5.0835],\n",
            "        [ -3.8707, -14.3818],\n",
            "        [-10.9802,  -4.7306],\n",
            "        [-11.9792,  -3.5115],\n",
            "        [-11.6509,  -3.4768],\n",
            "        [-11.8358,  -3.2520],\n",
            "        [-10.7440,  -4.5834],\n",
            "        [ -7.7270,  -7.7610],\n",
            "        [ -3.6355, -14.9987],\n",
            "        [ -4.8534, -12.1238],\n",
            "        [-10.8642,  -3.9257],\n",
            "        [ -5.4287, -10.8893],\n",
            "        [-11.6616,  -3.5692],\n",
            "        [ -4.7463, -13.0329],\n",
            "        [ -3.5312, -15.0226],\n",
            "        [ -2.2609, -17.9618],\n",
            "        [-10.6019,  -4.6174],\n",
            "        [ -4.7650, -12.3395],\n",
            "        [-12.3932,  -3.0434],\n",
            "        [-10.5070,  -4.8831],\n",
            "        [-13.0662,  -2.4344],\n",
            "        [ -4.3839, -13.1281]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[ -3.9498, -13.6407],\n",
            "        [-11.4327,  -3.7092],\n",
            "        [ -3.0511, -15.5089],\n",
            "        [-11.1654,  -3.9669],\n",
            "        [-11.4222,  -3.6891],\n",
            "        [-12.0287,  -3.2598],\n",
            "        [ -2.0857, -17.6023],\n",
            "        [-10.2872,  -4.6777],\n",
            "        [ -1.8419, -18.2791],\n",
            "        [ -2.2699, -17.2733],\n",
            "        [ -4.6419, -12.1136],\n",
            "        [-10.5597,  -4.2722],\n",
            "        [-12.1861,  -3.2133],\n",
            "        [ -3.9431, -13.5310],\n",
            "        [-10.0772,  -6.5017],\n",
            "        [ -5.2787, -11.2566],\n",
            "        [ -3.3509, -14.8312],\n",
            "        [-12.5329,  -3.5726],\n",
            "        [-10.2418,  -5.2634],\n",
            "        [ -2.4249, -17.0090],\n",
            "        [-13.0535,  -2.8280],\n",
            "        [-11.1674,  -3.8987],\n",
            "        [-10.0767,  -4.6172],\n",
            "        [-11.4957,  -3.6282],\n",
            "        [-10.8810,  -4.5390],\n",
            "        [-11.2074,  -4.6757],\n",
            "        [ -2.7180, -16.2761],\n",
            "        [-13.6690,  -2.2884],\n",
            "        [ -8.9034,  -7.5997],\n",
            "        [ -2.2956, -17.2781],\n",
            "        [-10.9811,  -3.9538],\n",
            "        [-11.2020,  -3.9386],\n",
            "        [-13.0138,  -2.6434],\n",
            "        [-10.7359,  -4.1980],\n",
            "        [-11.5905,  -4.6422],\n",
            "        [ -5.0937, -11.0870],\n",
            "        [-10.7664,  -4.9322],\n",
            "        [-11.3965,  -3.7633],\n",
            "        [ -3.0070, -15.8521],\n",
            "        [-10.0824,  -6.0661],\n",
            "        [-11.3504,  -3.7803],\n",
            "        [ -4.8849, -11.9909],\n",
            "        [ -9.4743,  -5.1819],\n",
            "        [ -4.0504, -13.2682],\n",
            "        [-12.9352,  -2.6409],\n",
            "        [ -4.2675, -12.7969],\n",
            "        [-11.3590,  -3.7233],\n",
            "        [-13.1602,  -2.5270],\n",
            "        [-10.9471,  -4.0381],\n",
            "        [-10.8501,  -4.1978],\n",
            "        [ -9.3672,  -5.2105],\n",
            "        [ -4.9954, -11.3216],\n",
            "        [-11.8803,  -3.5237],\n",
            "        [-10.8420,  -4.1222],\n",
            "        [ -2.3315, -17.1190],\n",
            "        [ -4.0144, -13.4314],\n",
            "        [-10.1447,  -4.6993],\n",
            "        [ -5.2246, -10.7477],\n",
            "        [-10.6934,  -4.2796],\n",
            "        [-10.9891,  -3.9803],\n",
            "        [ -8.8648,  -6.5851],\n",
            "        [-11.1061,  -5.2750],\n",
            "        [-12.1657,  -3.1981],\n",
            "        [-10.5603,  -4.3171]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[ -9.9791,  -4.6091],\n",
            "        [ -4.6522, -12.4188],\n",
            "        [-12.1609,  -3.1349],\n",
            "        [-10.8162,  -4.1864],\n",
            "        [ -4.1185, -13.5907],\n",
            "        [ -2.5001, -17.0712],\n",
            "        [ -3.0613, -15.9107],\n",
            "        [ -3.6998, -14.5344],\n",
            "        [-12.1482,  -3.3488],\n",
            "        [ -5.0398, -11.4866],\n",
            "        [ -3.9537, -14.6807],\n",
            "        [-13.1816,  -2.4548],\n",
            "        [-11.5714,  -3.8624],\n",
            "        [ -5.1990, -11.2431],\n",
            "        [ -3.1693, -15.7219],\n",
            "        [ -3.5925, -14.7575],\n",
            "        [ -2.5779, -16.9227],\n",
            "        [-12.0190,  -3.1963],\n",
            "        [-11.5222,  -3.5455],\n",
            "        [ -7.1877,  -9.6152],\n",
            "        [ -2.9494, -16.1143],\n",
            "        [ -2.0693, -18.1167],\n",
            "        [-10.4899,  -5.3865],\n",
            "        [ -8.8578,  -6.3603],\n",
            "        [ -4.2959, -13.3015],\n",
            "        [ -2.6548, -17.0223],\n",
            "        [ -8.2319,  -6.4782],\n",
            "        [ -4.7518, -12.2319],\n",
            "        [ -3.7731, -14.3176],\n",
            "        [-11.3490,  -3.6864],\n",
            "        [ -8.2815,  -6.0836],\n",
            "        [ -4.6276, -12.4640],\n",
            "        [ -4.7503, -12.1474],\n",
            "        [-12.0851,  -3.1812],\n",
            "        [ -4.1813, -13.3347],\n",
            "        [ -5.2505, -12.6819],\n",
            "        [ -3.8665, -14.0727],\n",
            "        [-11.4155,  -3.5795],\n",
            "        [ -3.3605, -15.2307],\n",
            "        [ -4.0655, -14.1666],\n",
            "        [ -2.6590, -17.1865],\n",
            "        [ -3.2921, -15.6086],\n",
            "        [-11.7480,  -3.4510],\n",
            "        [-10.8659,  -4.0395],\n",
            "        [ -9.7488,  -5.1467],\n",
            "        [-11.3462,  -4.8639],\n",
            "        [ -9.2320,  -6.2699],\n",
            "        [-11.4439,  -3.7981],\n",
            "        [ -3.7954, -14.2549],\n",
            "        [ -4.6681, -12.4519],\n",
            "        [ -4.5266, -12.7839],\n",
            "        [-11.7113,  -3.4662],\n",
            "        [-10.8158,  -4.0766],\n",
            "        [ -7.7313,  -9.0737],\n",
            "        [ -5.1376, -11.4216],\n",
            "        [ -4.4807, -12.7283],\n",
            "        [ -4.0369, -13.7640],\n",
            "        [-11.1412,  -5.2038],\n",
            "        [-14.3024,  -1.6885],\n",
            "        [-14.4686,  -1.5877],\n",
            "        [-10.2230,  -4.4765],\n",
            "        [ -2.9782, -16.3655],\n",
            "        [-11.3290,  -3.7558],\n",
            "        [-12.4506,  -2.9984]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[ -4.4209, -13.8306],\n",
            "        [-11.6104,  -3.5333],\n",
            "        [-11.1961,  -5.0409],\n",
            "        [-12.0267,  -3.3780],\n",
            "        [ -2.2575, -18.7108],\n",
            "        [-12.3540,  -3.0923],\n",
            "        [ -6.1343, -10.7370],\n",
            "        [ -3.8320, -15.1405],\n",
            "        [ -4.4320, -13.6446],\n",
            "        [ -3.8723, -14.9428],\n",
            "        [ -3.0485, -16.8009],\n",
            "        [ -4.5425, -13.4187],\n",
            "        [-11.5795,  -4.1835],\n",
            "        [ -4.3769, -13.8362],\n",
            "        [ -5.1125, -12.1573],\n",
            "        [ -5.2920, -11.7790],\n",
            "        [-11.6793,  -3.9032],\n",
            "        [-11.7537,  -3.7171],\n",
            "        [-12.3247,  -3.3928],\n",
            "        [ -2.8132, -17.2714],\n",
            "        [ -2.6407, -17.6296],\n",
            "        [ -3.1830, -16.5805],\n",
            "        [ -4.6453, -13.3313],\n",
            "        [ -3.2431, -16.2770],\n",
            "        [ -4.3722, -13.7992],\n",
            "        [-11.4531,  -4.1567],\n",
            "        [-12.8101,  -2.8126],\n",
            "        [-12.0504,  -3.9123],\n",
            "        [ -4.1783, -14.2108],\n",
            "        [-12.4777,  -2.9782],\n",
            "        [-13.5362,  -2.4075],\n",
            "        [-13.7286,  -2.3827],\n",
            "        [ -3.6192, -15.4759],\n",
            "        [ -4.7944, -12.8947],\n",
            "        [ -3.4714, -15.8389],\n",
            "        [ -3.4112, -16.0232],\n",
            "        [-12.1673,  -3.2660],\n",
            "        [-12.7912,  -2.9887],\n",
            "        [ -2.6075, -17.7990],\n",
            "        [-10.3520,  -5.0088],\n",
            "        [-10.7838,  -4.8955],\n",
            "        [ -4.4563, -13.5884],\n",
            "        [-12.4932,  -3.5926],\n",
            "        [-11.9878,  -3.3549],\n",
            "        [-10.6352,  -4.2470],\n",
            "        [ -4.9099, -12.5392],\n",
            "        [ -3.6744, -15.5726],\n",
            "        [-11.4363,  -3.7294],\n",
            "        [ -3.3054, -16.1220],\n",
            "        [ -4.7884, -12.8790],\n",
            "        [-11.7681,  -3.5001],\n",
            "        [ -3.6425, -15.4562],\n",
            "        [-11.8656,  -3.5520],\n",
            "        [-12.0273,  -3.8539],\n",
            "        [ -4.9168, -12.5476],\n",
            "        [-14.1830,  -1.8670],\n",
            "        [ -4.8666, -12.8412],\n",
            "        [ -3.2633, -16.2291],\n",
            "        [ -4.6262, -13.2578],\n",
            "        [ -2.4562, -18.1812],\n",
            "        [-10.7143,  -4.1951],\n",
            "        [-10.4758,  -4.6747],\n",
            "        [ -4.7425, -12.9973],\n",
            "        [ -3.0051, -16.9878]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[-11.1657,  -3.9020],\n",
            "        [ -4.3075, -13.4738],\n",
            "        [ -4.2150, -14.3328],\n",
            "        [ -4.5781, -12.9072],\n",
            "        [ -1.8700, -18.7304],\n",
            "        [ -4.0936, -13.8805],\n",
            "        [-12.1312,  -3.0743],\n",
            "        [ -3.7817, -14.4918],\n",
            "        [ -3.9798, -14.0357],\n",
            "        [-10.9408,  -3.9276],\n",
            "        [-12.2435,  -3.0693],\n",
            "        [ -4.5593, -13.0201],\n",
            "        [ -3.8404, -14.3746],\n",
            "        [ -3.8078, -14.4834],\n",
            "        [ -9.7156,  -4.7378],\n",
            "        [-11.6663,  -3.3902],\n",
            "        [ -2.6410, -17.0491],\n",
            "        [ -4.5408, -12.8691],\n",
            "        [-11.9038,  -3.2208],\n",
            "        [ -4.2432, -14.8507],\n",
            "        [ -9.8407,  -4.8147],\n",
            "        [ -2.7189, -16.8012],\n",
            "        [ -4.4994, -12.9041],\n",
            "        [-12.3758,  -2.9090],\n",
            "        [ -2.4259, -17.5509],\n",
            "        [ -4.3339, -13.2976],\n",
            "        [ -2.2622, -17.9242],\n",
            "        [ -4.4160, -13.6463],\n",
            "        [ -5.8640, -11.0895],\n",
            "        [-14.7435,  -1.3377],\n",
            "        [-10.5449,  -4.6069],\n",
            "        [-10.5350,  -4.8910],\n",
            "        [-13.6560,  -2.1241],\n",
            "        [ -8.9288,  -5.8770],\n",
            "        [-11.7660,  -3.3503],\n",
            "        [-13.3288,  -2.4126],\n",
            "        [ -4.7820, -12.3294],\n",
            "        [ -3.9086, -14.2410],\n",
            "        [ -4.4044, -13.2648],\n",
            "        [ -2.8988, -16.4389],\n",
            "        [-12.8521,  -2.6749],\n",
            "        [-10.9112,  -4.5134],\n",
            "        [ -4.5884, -12.7742],\n",
            "        [ -3.9144, -14.2552],\n",
            "        [ -2.7976, -16.7959],\n",
            "        [ -5.4181, -10.8452],\n",
            "        [ -4.4007, -13.1025],\n",
            "        [-11.6951,  -3.3814],\n",
            "        [-10.0841,  -4.6390],\n",
            "        [ -2.3323, -17.7834],\n",
            "        [-11.0773,  -3.7995],\n",
            "        [-11.5932,  -3.7559],\n",
            "        [-10.8902,  -4.5483],\n",
            "        [-10.5687,  -4.1658]], grad_fn=<LogSoftmaxBackward>)\n",
            "Testing accuracy: 2017/2038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHzQDnLOjorv"
      },
      "source": [
        "# not needed to update the data\n",
        "# visualize the results\n",
        "fig=plt.figure()\n",
        "ax = fig.add_axes([0, 0, 1, 1])\n",
        "ax.bar(epochs, height=accuracy_graphing)\n",
        "ax.set_ylabel('Number Correct')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_title('Binary accuracy by epoch with training')\n",
        "ax.set_yticks(np.arange(0, 13001, 1000))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}